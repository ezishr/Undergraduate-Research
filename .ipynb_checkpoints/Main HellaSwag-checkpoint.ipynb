{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d2fba69-f501-49ba-8889-ed49583db0c4",
   "metadata": {},
   "source": [
    "#### HellaSwag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00214223-41a4-4566-8227-4d2e1df4ed9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install import-ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4624b86-8b23-431e-baca-bacbc4a289ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../Modules')\n",
    "sys.path.append('../Modules/Processors from Prof')\n",
    "\n",
    "from Packages import *\n",
    "import contextlib\n",
    "\n",
    "\n",
    "# Import Jupyter Notebook files\n",
    "with contextlib.redirect_stdout(None):\n",
    "    #ToDo import the correct data processor: JSON or CSV\n",
    "    from ipynb.fs.full.Json_Processor import *\n",
    "    from ipynb.fs.full.CSV_Processor import *\n",
    "    from ipynb.fs.full.Reading_Level import *\n",
    "    from ipynb.fs.full.Word_Processing import *\n",
    "    from ipynb.fs.full.Utilities import *\n",
    "    from Wordcloud import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "672b3191-3327-4b88-be40-1662f6d77fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_questions(questions):\n",
    "    analysis = dict()\n",
    "    activity_labels = set()\n",
    "    for question in questions:\n",
    "        activity_labels.add(question[\"activity_label\"])\n",
    "\n",
    "    analysis[\"activity_labels\"] = activity_labels\n",
    "    return analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bcb52f5-c928-4519-bd84-7769dafdc7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_question(questions, source_id = None):\n",
    "    '''\n",
    "    Extract a random question \n",
    "    @param questions list: a list of dictionaries, each a question. \n",
    "    @param source_id str: the desired source of the question. Defaults to None for don't care.  \n",
    "    @return dictionary: The random question as a dictionary\n",
    "    '''\n",
    "    if source_id == None:\n",
    "        return random.choice(questions)\n",
    "    else:\n",
    "        while True:\n",
    "            question = random.choice(questions)\n",
    "            if source_id in question[\"source_id\"]:\n",
    "                return question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0328a99a-27d1-4760-99d7-4eb951c35f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_question(question):\n",
    "    '''\n",
    "    Print a question\n",
    "    @param question dictionary: The question to be printed\n",
    "    @return: None\n",
    "    '''\n",
    "    print(\"Activity Label:\", question[\"activity_label\"])\n",
    "    print(\"Full Context:\", question[\"ctx_a\"])\n",
    "    print(\"Source_id:\", question[\"source_id\"])\n",
    "    try:\n",
    "        print(\"Label (correct answer, zero-based):\", question[\"label\"])\n",
    "    except:\n",
    "        print(\"Label: not provided, probably because this is the test dataset\")\n",
    "    print(\"Endings:\")\n",
    "    idx = 0\n",
    "    for ending in question[\"endings\"]:\n",
    "        print(idx, \":\", question[\"ctx_b\"] + \" \" + ending)\n",
    "        idx += 1\n",
    "    \n",
    "    #for key in question.keys():\n",
    "    #    print(key,\":\",question[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4de3ec58-fd5e-4cdb-9245-26a6d292b517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_text_from_questions(questions):\n",
    "    '''\n",
    "    Build one long text string from all the questions.\n",
    "    This logic will vary based on the architecture of the benchmark questions.\n",
    "    @param questions list: list of dictionaries, one dictionary per question.\n",
    "    @return String: The text string\n",
    "    '''\n",
    "    \n",
    "    # dict_keys(['ind', 'activity_label', 'ctx_a', 'ctx_b', 'ctx', 'split', 'split_type', 'label', 'endings', 'source_id']\n",
    "    #print(\"HellaSwag keys are:\")\n",
    "    #print(questions[0].keys())\n",
    "    text = \"\"\n",
    "    # Contrive the prompt as a complete sentence and each of the possible completions as a complete sentence.\n",
    "    for question in questions:\n",
    "        text += \" \" + str(question[\"ctx_a\"])\n",
    "        for ending in question[\"endings\"]:\n",
    "            text += \" \" + question[\"ctx_b\"] + \" \" + ending\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dec1236f-3e33-45a0-8979-b2e13df70a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_readability_indices(questions, verbose = True):\n",
    "    '''\n",
    "    Compute readability values using our Reading_Level class\n",
    "    @param questions list: A list of dictionaries, one dictionary per benchmark question\n",
    "    @param verbose bool: True if this function should print the computed values\n",
    "    @return dictionary: key is the readability metric, value is the corresponding score\n",
    "    '''\n",
    "    text = build_text_from_questions(questions)\n",
    "    indices = Reading_Level.compute_readability_indices(text)\n",
    "\n",
    "    if (verbose):\n",
    "        print(\"Readability Indices:\")\n",
    "        for index, score in indices.items():\n",
    "            print(f\"{index}: {score:.2f}\")\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "422c8b4c-8845-49fe-94d1-b8ec8f79d692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wordcloud(questions):\n",
    "    '''\n",
    "    Build a word cloud based on the questions in a benchmark\n",
    "    @param questions list: List of dictionaries, each dictionary os a question from the benchmark\n",
    "    @return None\n",
    "    '''\n",
    "    text = build_text_from_questions(questions)\n",
    "\n",
    "    wordcloud = Wordcloud()\n",
    "    wordcloud.generate01(text, myStopwords={\"Numerical\", \"options\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c38a148-dda8-4353-831d-da518c932f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_word_frequency(questions, verbose = True, min_percentage = 1.0):\n",
    "    '''\n",
    "    Compute the word frequencies in the benchmark questions\n",
    "    @param questions list: List of dictionaries, each dictionary os a question from the benchmark\n",
    "    @param verbose bool: True if the function should print the word frequencies\n",
    "    @param min_percentage float: the smallest percentage that should be printed. Defaults to 1.0\n",
    "    @return set (dictionary, float): ({word:# of times that word appears over all questions}, total words)\n",
    "    '''\n",
    "    text = build_text_from_questions(questions)\n",
    "\n",
    "    word_frequency, count = Word_Processing.compute_word_frequency(text)\n",
    "    sorted_word_frequency = {k: v for k, v in sorted(word_frequency.items(), key=lambda item: item[1], reverse = True)}\n",
    "    count = float(count)\n",
    "    if verbose:\n",
    "        print(\"Word Frequency:\")\n",
    "        #for key in [key for key in sorted_word_frequency.keys()][:5]:\n",
    "        for key in sorted_word_frequency.keys():\n",
    "            percentage = (sorted_word_frequency[key] / count)* 100\n",
    "            if percentage >= min_percentage:\n",
    "                print(key, \":\", sorted_word_frequency[key], \",\", '{0:.2f}'.format(percentage))\n",
    "\n",
    "    return (sorted_word_frequency, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdf419d2-76de-46d3-91e4-f40d72f36e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_longest_words(questions, verbose = True):\n",
    "    '''\n",
    "    Compute the longest words appearing across all the questions\n",
    "    @param questions list: List of dictionaries, each dictionary is a question from the benchmark\n",
    "    @param verbose bool: True if the function should print the words\n",
    "    @return set (dictionary, float): ({word:# of times that word appears over all questions}, total words)    \n",
    "    '''\n",
    "\n",
    "    text = build_text_from_questions(questions)\n",
    "    #print(\"**********************\")\n",
    "    #print(text.split())\n",
    "    #print(\"**********************\")\n",
    "\n",
    "    word_length, count = Word_Processing.compute_longest_words(text, min_length = 12)\n",
    "    #sorted_word_lengths = {k: v for k, v in sorted(word_length.items(), key=lambda item: item[1], reverse = True)}\n",
    "    sorted_word_lengths = {k: v for k, v in sorted(word_length.items(), key=lambda item: len(item[0]), reverse = True)}\n",
    "    count = float(count)\n",
    "    if verbose:\n",
    "        print(\"Longest Words:\")\n",
    "        for key in list(sorted_word_lengths.keys())[0:25]:\n",
    "            print(len(key), \", \", key, \":\", sorted_word_lengths[key], \",\", '{0:.2f}'.format((sorted_word_lengths[key] / count)* 100))\n",
    "\n",
    "    return (sorted_word_lengths, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f65e7268-6bca-4b9c-8bcb-a7de422e1aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_word(questions, word):\n",
    "    return Word_Processing.find_word(questions, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f06459f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_missing_words(benchmark_name, questions, verbose = True, very_verbose = False):\n",
    "    \"\"\"\n",
    "    Find words not in the English dictionary. Numbers are ignored in this function.\n",
    "    @param questions dictionary: The data to be processed\n",
    "    @param verbose bool: If true, print some information about the first 10 words not found in the dictionary. Default to True\n",
    "    @param very_verbose bool: If true, print the questions containing the first 10 words that were not found in the dictionary. Default to False.\n",
    "    \n",
    "    @return dictionary: The unique words. key and value are both the unique word\n",
    "    \"\"\"\n",
    "    text = build_text_from_questions(questions)\n",
    "    print(\"Text built...\")\n",
    "    english = Word_Processing.load_dictionary()\n",
    "    print(\"Dictionary loaded...\")\n",
    "\n",
    "    words = Word_Processing.split_text(text)\n",
    "    print(\"Text split...\")\n",
    "    words_not_found = dict()\n",
    "    print(\"Processing word list...\")\n",
    "    for word in words:\n",
    "        try:\n",
    "            # If this fails, the word is not a number and we will add it to the dictionary of missing words.\n",
    "            tmp = float(word)\n",
    "        except:\n",
    "            if word.upper() not in english:\n",
    "                words_not_found[word] = word\n",
    "                #words_not_found.add(word)\n",
    "    # Write all the missing words to a text file\n",
    "    write_dict_keys_to_file(words_not_found, \".\\\\\" + benchmark_name + \"\\\\results\\\\words_not_in_dictionary.txt\", length = len(words_not_found))\n",
    "\n",
    "    if verbose:\n",
    "        print(len(words_not_found), \"words not in dictionary\")\n",
    "        print(\"First 10 words not in dictionary:\")\n",
    "        for i, key in enumerate(words_not_found.keys()):\n",
    "            if i >= 10:\n",
    "              break\n",
    "            print(key)\n",
    "    if very_verbose:\n",
    "        print(\"First 10 words not in dictionary and the questions those words appear in:\")\n",
    "        for i, key in enumerate(words_not_found.keys()):\n",
    "            if i >= 10:\n",
    "                break\n",
    "            print(key)\n",
    "            print(key , \"found in\", find_word(questions, key))\n",
    "    \n",
    "    return words_not_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "46dc5ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo():\n",
    "\n",
    "    benchmark_name = \"HellaSwag\"\n",
    "    input_files = ['hellaswag_train.json']\n",
    "    # input_file_path = 'data/hellaswag_train.json'\n",
    "    # output_file_path = 'data/output.json'\n",
    "    # question_path = \"data/\"\n",
    "    \n",
    "    json_processor = HellaSwag_Json_Processor(benchmark_name, input_files)\n",
    "    questions = json_processor.read_data()\n",
    "\n",
    "    # print(len(questions), \"questions read from\", input_file_path)\n",
    "    # print(len(questions), \"questions read from\", len(json_files), \"files in\", question_path)\n",
    "\n",
    "    random_question = get_random_question(questions)\n",
    "\n",
    "    words_not_found = find_missing_words(benchmark_name, questions)\n",
    "\n",
    "    print(\"Random question:\")\n",
    "    print_question(random_question)\n",
    "    \n",
    "    compute_readability_indices(benchmark_name, questions)\n",
    "    \n",
    "    generate_wordcloud(benchmark_name, questions)\n",
    "\n",
    "    compute_word_frequency(benchmark_name, questions)\n",
    "    \n",
    "    compute_longest_words(benchmark_name, questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b365fda8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'ctx_a'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdemo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[32], line 17\u001b[0m, in \u001b[0;36mdemo\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# print(len(questions), \"questions read from\", input_file_path)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# print(len(questions), \"questions read from\", len(json_files), \"files in\", question_path)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m random_question \u001b[38;5;241m=\u001b[39m get_random_question(questions)\n\u001b[0;32m---> 17\u001b[0m words_not_found \u001b[38;5;241m=\u001b[39m \u001b[43mfind_missing_words\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbenchmark_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRandom question:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m print_question(random_question)\n",
      "Cell \u001b[0;32mIn[23], line 10\u001b[0m, in \u001b[0;36mfind_missing_words\u001b[0;34m(benchmark_name, questions, verbose, very_verbose)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_missing_words\u001b[39m(benchmark_name, questions, verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, very_verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    Find words not in the English dictionary. Numbers are ignored in this function.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    @param questions dictionary: The data to be processed\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m    @return dictionary: The unique words. key and value are both the unique word\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_text_from_questions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mText built...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m     english \u001b[38;5;241m=\u001b[39m Word_Processing\u001b[38;5;241m.\u001b[39mload_dictionary()\n",
      "Cell \u001b[0;32mIn[6], line 15\u001b[0m, in \u001b[0;36mbuild_text_from_questions\u001b[0;34m(questions)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Contrive the prompt as a complete sentence and each of the possible completions as a complete sentence.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m question \u001b[38;5;129;01min\u001b[39;00m questions:\n\u001b[0;32m---> 15\u001b[0m     text \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[43mquestion\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mctx_a\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ending \u001b[38;5;129;01min\u001b[39;00m question[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mendings\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m     17\u001b[0m         text \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m question[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mctx_b\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m ending\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ctx_a'"
     ]
    }
   ],
   "source": [
    "demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb974de3-9515-4a2c-ae9c-29e3adbbbef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4da59344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Then, the man writes over the snow covering the window of a car, and a woman wearing winter clothes smiles.',\n",
       " 'topic': 'Removing ice from car',\n",
       " 'target': 'then , the man adds wax to the windshield and cuts it.\\nthen , a person board a ski lift, while two men supporting the head of the person wearing winter clothes snow as the we girls sled.\\nthen , the man puts on a christmas coat, knitted with netting.\\nthen , the man continues removing the snow on his car.\\n'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4729d866",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
