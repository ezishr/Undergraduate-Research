{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../Modules/')\n",
    "sys.path.append('../../Modules/Processors from Prof')\n",
    "from Packages import *\n",
    "from My_Json_processor import *\n",
    "from My_Utilities_processor import *\n",
    "\n",
    "# Import processors from Prof\n",
    "from ipynb.fs.full.Utilities import *\n",
    "from ipynb.fs.full.Json_Processor import *\n",
    "from ipynb.fs.full.CSV_Processor import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get GEMINI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "\n",
    "# os.environ[\"GEMINI_API_KEY\"] = \"AIzaSyAP8QMlHxlLhX518vKYJbd3bQuKZlb6pD0\"\n",
    "os.environ[\"GEMINI_API_KEY\"] = \"AIzaSyCM-GWMhMPoBZpvlXWqKr5nKnY02OIVdf4\"\n",
    "\n",
    "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get GROQ API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
    "\n",
    "client = Groq(\n",
    "    api_key=os.environ['GROQ_API_KEY'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to generate GEMINI Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = Big_Bench_Json_Processor_my('Big Bench Hard', 'boolean_expressions.json').convert_df().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD\n",
    "\n",
    "model=genai.GenerativeModel(\n",
    "  model_name=\"gemini-1.5-flash\",\n",
    "  system_instruction=\"You only answer True or False to the following questions.\",)\n",
    "\n",
    "sample['gemini_output'] = None\n",
    "\n",
    "for i in range(len(sample)):\n",
    "    response = model.generate_content(sample['input'][i])\n",
    "    print(response.text)\n",
    "    sample.loc[i, 'gemini_output'] = response.text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = []\n",
    "\n",
    "for i in range(len(sample)):\n",
    "    content.append(sample['input'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel(\n",
    "    model_name = \"gemini-1.5-flash\",\n",
    "    system_instruction=\"You only answer True or False to the following questions.\",\n",
    ")\n",
    "\n",
    "\n",
    "response = model.generate_content(content)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GROQ\n",
    "\n",
    "The function is to generate GROQ answers for the given data.\n",
    "\n",
    "@params: data(initial dataframe), system_message(message for Gemini system)\n",
    "@return: sample(dataframe with gemini_output column)\n",
    "\"\"\"\n",
    "\n",
    "def groq(data, system_message, model_name):\n",
    "\n",
    "    import time\n",
    "    total_requests = 0\n",
    "    successful_requests = 0\n",
    "    client = Groq(api_key=os.environ['GROQ_API_KEY'],)\n",
    "\n",
    "    sample = data.copy()\n",
    "    sample[model_name] = None\n",
    "\n",
    "\n",
    "    for i in range(len(sample)):\n",
    "        success = False\n",
    "        retries = 3\n",
    "\n",
    "        while not success and retries > 0:\n",
    "            try:\n",
    "                total_requests += 1\n",
    "                \n",
    "                # Make a request to the GROQ API\n",
    "                chat_completion = client.chat.completions.create(\n",
    "                    messages=[\n",
    "                        {\n",
    "                            \"role\":\"user\",\n",
    "                            \"content\": sample.loc[i, 'input']\n",
    "                        },\n",
    "                        {\n",
    "                            'role': 'system',\n",
    "                            'content': system_message\n",
    "                        }\n",
    "                    ],\n",
    "                    model = model_name\n",
    "                )\n",
    "\n",
    "                response = chat_completion.choices[0].message.content\n",
    "\n",
    "                sample.loc[i, model_name] = response.strip()\n",
    "                success = True\n",
    "                successful_requests += 1\n",
    "                # print(response)\n",
    "                time.sleep(5)\n",
    "\n",
    "            except Exception as e:\n",
    "                # print(f\"Error: {e}\")\n",
    "                retries -= 1\n",
    "                time.sleep(5)\n",
    "                total_requests += 1\n",
    "\n",
    "    print(f\"Total requests made: {total_requests}\")\n",
    "    print(f\"Successful requests: {successful_requests}\")\n",
    "\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GROQ\n",
    "\n",
    "The function is to generate GROQ answers for the given data ROWS.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def groq_line_generate(raw_dataset, output_dataset, start_idx, end_idx, system_message, model_name):\n",
    "    sample = raw_dataset.loc[start_idx:end_idx, ].copy()\n",
    "    sample.reset_index(drop=True, inplace=True)\n",
    "    groq_sample = groq(sample, system_message, model_name)\n",
    "    output_dataset.loc[start_idx:end_idx, \"llama_output\"] = groq_sample[model_name].values\n",
    "    return output_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GEMINI\n",
    "\n",
    "The function is to generate Gemini answers for the given data.\n",
    "\n",
    "@params: data(initial dataframe), system_message(message for Gemini system)\n",
    "@return: sample(dataframe with gemini_output column)\n",
    "\"\"\"\n",
    "\n",
    "def gemini_generator(data, system_message):\n",
    "\n",
    "    import time\n",
    "    total_requests = 0\n",
    "    successful_requests = 0\n",
    "\n",
    "\n",
    "    model=genai.GenerativeModel(\n",
    "    model_name=\"gemini-1.5-flash\",\n",
    "    system_instruction=system_message,\n",
    "    )\n",
    "\n",
    "    sample = data.copy()\n",
    "    sample['gemini_output'] = None\n",
    "\n",
    "    for i in range(len(sample)):\n",
    "        success = False\n",
    "        retries = 3\n",
    "\n",
    "        while not success and retries > 0:\n",
    "            try:\n",
    "                total_requests += 1\n",
    "\n",
    "                # Make API request\n",
    "                response = model.generate_content(sample['input'][i])\n",
    "                # print(response.text)\n",
    "                sample.loc[i, 'gemini_output'] = response.text.strip()\n",
    "                success = True\n",
    "                successful_requests += 1\n",
    "                time.sleep(5)\n",
    "\n",
    "            except Exception as e:\n",
    "                # print(f\"Error: {e}\")\n",
    "                retries -= 1\n",
    "                time.sleep(5)\n",
    "                total_requests += 1\n",
    "\n",
    "    print(f\"Total requests made: {total_requests}\")\n",
    "    print(f\"Successful requests: {successful_requests}\")\n",
    "\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The function is to export the dataframe to a csv file.\n",
    "\n",
    "@params: folder_name(name of the folder), df(dataframe)\n",
    "@return: True if the dataframe is exported successfully, False otherwise\n",
    "\"\"\"\n",
    "\n",
    "def df_to_csv(folder_name, df, file_name, output_col):\n",
    "    base_path = r'/Users/ezishr/Library/CloudStorage/OneDrive-UniversityofCincinnati/Undergraduate Research/Check points'\n",
    "    if (df[output_col].isnull().sum() == 0) or (df[output_col].isnull().sum() == 0):\n",
    "        file_path = os.path.join(base_path, folder_name, f'{file_name}.csv')\n",
    "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "        df.to_csv(file_path, index=False)\n",
    "        print(f\"DataFrame exported successfully to {file_path}!\")\n",
    "    else:\n",
    "        print(\"Export failed: DataFrame contains null values in 'gemini_output'.\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Testing - boolean_expressions.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "boolean_df = Big_Bench_Json_Processor_my('Big Bench Hard', 'boolean_expressions.json').convert_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(boolean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boolean_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GEMINI applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample0_10 = boolean_df.head(10)\n",
    "gemini_sample0_10 = gemini_generator(sample0_10, \"You only answer True or False to the following questions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "total_requests = 0\n",
    "successful_requests = 0\n",
    "\n",
    "\n",
    "model=genai.GenerativeModel(\n",
    "  model_name=\"gemini-1.5-flash\",\n",
    "  system_instruction=\"You only answer True or False to the following questions.\",\n",
    "  )\n",
    "\n",
    "sample2 = boolean_df.copy()\n",
    "sample2['gemini_output'] = None\n",
    "\n",
    "for i in range(len(sample2)):\n",
    "    success = False\n",
    "    retries = 3\n",
    "\n",
    "    while not success and retries > 0:\n",
    "        try:\n",
    "            total_requests += 1\n",
    "\n",
    "            # Make API request\n",
    "            response = model.generate_content(sample2['input'][i])\n",
    "            # print(response.text)\n",
    "            sample2.loc[i, 'gemini_output'] = response.text.strip()\n",
    "            success = True\n",
    "            successful_requests += 1\n",
    "            time.sleep(5)\n",
    "\n",
    "        except Exception as e:\n",
    "            # print(f\"Error: {e}\")\n",
    "            retries -= 1\n",
    "            time.sleep(5)\n",
    "            total_requests += 1 \n",
    "\n",
    "\n",
    "\n",
    "print(f\"Total requests made: {total_requests}\")\n",
    "print(f\"Successful requests: {successful_requests}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample2['gemini_output'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "\n",
    "model=genai.GenerativeModel(\n",
    "  model_name=\"gemini-1.5-flash\",\n",
    "  system_instruction=\"You only answer True or False to the following questions.\",\n",
    "  )\n",
    "\n",
    "\n",
    "total_requests = 0\n",
    "\n",
    "successful_requests = 0\n",
    "unsuccesful_requests = 0\n",
    "\n",
    "sample3 = boolean_df.copy()\n",
    "sample3['gemini_output'] = None\n",
    "\n",
    "while sample3['gemini_output'].isna().sum() > 0:\n",
    "    for i in range(len(sample2)):\n",
    "        if sample3['gemini_output'][i] == None:\n",
    "            try:\n",
    "                response = model.generate_content(sample2['input'][i])\n",
    "                # print(response.text)\n",
    "                sample3.loc[i, 'gemini_output'] = response.text.strip()\n",
    "                successful_requests += 1\n",
    "                time.sleep(5)\n",
    "\n",
    "            except Exception as e:\n",
    "                # print(f\"Error: {e}\")\n",
    "                unsuccesful_requests += 1\n",
    "                time.sleep(5)\n",
    "\n",
    "            total_requests += 1\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "print(f\"Successful requests: {successful_requests}\")\n",
    "print(f\"Unsuccessful requests: {unsuccesful_requests}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_boolean_df = sample2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_boolean_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_csv(\"Big Bench Hard\", gemini_boolean_df, file_name = \"gemini_boolean_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GROQ Applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boolean_df.info()\n",
    "boolean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_boolean_df = boolean_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_boolean_df = groq_line_generate(raw_dataset = boolean_df, output_dataset = groq_boolean_df, start_idx=11, end_idx=19, system_message=\"You only answer True or False to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_boolean_df = groq_line_generate(raw_dataset = boolean_df, output_dataset = groq_boolean_df, start_idx=20, end_idx=29, system_message=\"You only answer True or False to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_boolean_df = groq_line_generate(raw_dataset = boolean_df, output_dataset = groq_boolean_df, start_idx=30, end_idx=39, system_message=\"You only answer True or False to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_boolean_df = groq_line_generate(raw_dataset = boolean_df, output_dataset = groq_boolean_df, start_idx=40, end_idx=49, system_message=\"You only answer True or False to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_boolean_df = groq_line_generate(raw_dataset = boolean_df, output_dataset = groq_boolean_df, start_idx=50, end_idx=59, system_message=\"You only answer True or False to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_boolean_df = groq_line_generate(raw_dataset = boolean_df, output_dataset = groq_boolean_df, start_idx=60, end_idx=69, system_message=\"You only answer True or False to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_boolean_df = groq_line_generate(raw_dataset = boolean_df, output_dataset = groq_boolean_df, start_idx=70, end_idx=79, system_message=\"You only answer True or False to the following questions.\", model_name=\"llama-3.3-70b-versatile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_boolean_df = groq_line_generate(raw_dataset = boolean_df, output_dataset = groq_boolean_df, start_idx=80, end_idx=89, system_message=\"You only answer True or False to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_boolean_df = groq_line_generate(raw_dataset = boolean_df, output_dataset = groq_boolean_df, start_idx=90, end_idx=99, system_message=\"You only answer True or False to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_boolean_df = groq_line_generate(raw_dataset = boolean_df, output_dataset = groq_boolean_df, start_idx=100, end_idx=109, system_message=\"You only answer True or False to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_boolean_df = groq_line_generate(raw_dataset = boolean_df, output_dataset = groq_boolean_df, start_idx=110, end_idx=119, system_message=\"You only answer True or False to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_boolean_df = groq_line_generate(raw_dataset = boolean_df, output_dataset = groq_boolean_df, start_idx=120, end_idx=129, system_message=\"You only answer True or False to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_boolean_df = groq_line_generate(raw_dataset = boolean_df, output_dataset = groq_boolean_df, start_idx=130, end_idx=139, system_message=\"You only answer True or False to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_boolean_df = groq_line_generate(raw_dataset = boolean_df, output_dataset = groq_boolean_df, start_idx=140, end_idx=149, system_message=\"You only answer True or False to the following questions.\", model_name=\"llama-3.3-70b-versatile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_boolean_df = groq_line_generate(raw_dataset = boolean_df, output_dataset = groq_boolean_df, start_idx=150, end_idx=159, system_message=\"You only answer True or False to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_boolean_df = groq_line_generate(raw_dataset = boolean_df, output_dataset = groq_boolean_df, start_idx=160, end_idx=169, system_message=\"You only answer True or False to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_boolean_df = groq_line_generate(raw_dataset = boolean_df, output_dataset = groq_boolean_df, start_idx=170, end_idx=179, system_message=\"You only answer True or False to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_boolean_df = groq_line_generate(raw_dataset = boolean_df, output_dataset = groq_boolean_df, start_idx=180, end_idx=189, system_message=\"You only answer True or False to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_boolean_df = groq_line_generate(raw_dataset = boolean_df, output_dataset = groq_boolean_df, start_idx=190, end_idx=199, system_message=\"You only answer True or False to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_boolean_df = groq_line_generate(raw_dataset = boolean_df, output_dataset = groq_boolean_df, start_idx=200, end_idx=209, system_message=\"You only answer True or False to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_boolean_df = groq_line_generate(raw_dataset = boolean_df, output_dataset = groq_boolean_df, start_idx=210, end_idx=219, system_message=\"You only answer True or False to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_boolean_df = groq_line_generate(raw_dataset = boolean_df, output_dataset = groq_boolean_df, start_idx=220, end_idx=229, system_message=\"You only answer True or False to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_boolean_df = groq_line_generate(raw_dataset = boolean_df, output_dataset = groq_boolean_df, start_idx=230, end_idx=239, system_message=\"You only answer True or False to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_boolean_df = groq_line_generate(raw_dataset = boolean_df, output_dataset = groq_boolean_df, start_idx=240, end_idx=250, system_message=\"You only answer True or False to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_boolean_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_csv(\"Big Bench Hard\", groq_boolean_df, file_name = \"groq_boolean_df\", output_col = \"llama_output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Testing - causal_judgement.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_judgement = Big_Bench_Json_Processor_my('Big Bench Hard', 'causal_judgement.json').convert_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_judgement.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GEMINI Applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "total_requests = 0\n",
    "successful_requests = 0\n",
    "\n",
    "\n",
    "model=genai.GenerativeModel(\n",
    "  model_name=\"gemini-1.5-flash\",\n",
    "  system_instruction=\"You only answer Yes or No to the following questions.\",\n",
    "  )\n",
    "\n",
    "sample = causal_judgement.copy()\n",
    "sample['gemini_output'] = None\n",
    "\n",
    "for i in range(len(sample)):\n",
    "    success = False\n",
    "    retries = 3\n",
    "\n",
    "    while not success and retries > 0:\n",
    "        try:\n",
    "            total_requests += 1\n",
    "\n",
    "            # Make API request\n",
    "            response = model.generate_content(sample['input'][i])\n",
    "            # print(response.text)\n",
    "            sample.loc[i, 'gemini_output'] = response.text.strip()\n",
    "            success = True\n",
    "            successful_requests += 1\n",
    "            time.sleep(5)\n",
    "\n",
    "        except Exception as e:\n",
    "            # print(f\"Error: {e}\")\n",
    "            retries -= 1\n",
    "            time.sleep(5)\n",
    "            total_requests += 1 \n",
    "\n",
    "\n",
    "\n",
    "print(f\"Total requests made: {total_requests}\")\n",
    "print(f\"Successful requests: {successful_requests}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_causal_judgement = sample.copy()\n",
    "# gemini_causal_judgement.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_csv(\"Big Bench Hard\", gemini_causal_judgement, file_name = \"gemini_causal_judgement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GROQ - llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_causal_judgement = causal_judgement.copy()\n",
    "groq_causal_judgement[\"llama_output\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_causal_judgement = groq_line_generate(raw_dataset = causal_judgement, output_dataset = groq_causal_judgement, start_idx=0, end_idx=9, system_message=\"You only answer Yes or No to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_causal_judgement = groq_line_generate(raw_dataset = causal_judgement, output_dataset = groq_causal_judgement, start_idx=10, end_idx=19, system_message=\"You only answer Yes or No to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_causal_judgement = groq_line_generate(raw_dataset = causal_judgement, output_dataset = groq_causal_judgement, start_idx=20, end_idx=29, system_message=\"You only answer Yes or No to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_causal_judgement = groq_line_generate(raw_dataset = causal_judgement, output_dataset = groq_causal_judgement, start_idx=30, end_idx=39, system_message=\"You only answer Yes or No to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_causal_judgement = groq_line_generate(raw_dataset = causal_judgement, output_dataset = groq_causal_judgement, start_idx=40, end_idx=49, system_message=\"You only answer Yes or No to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_causal_judgement = groq_line_generate(raw_dataset = causal_judgement, output_dataset = groq_causal_judgement, start_idx=50, end_idx=59, system_message=\"You only answer Yes or No to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_causal_judgement = groq_line_generate(raw_dataset = causal_judgement, output_dataset = groq_causal_judgement, start_idx=60, end_idx=69, system_message=\"You only answer Yes or No to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_causal_judgement = groq_line_generate(raw_dataset = causal_judgement, output_dataset = groq_causal_judgement, start_idx=70, end_idx=79, system_message=\"You only answer Yes or No to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_causal_judgement = groq_line_generate(raw_dataset = causal_judgement, output_dataset = groq_causal_judgement, start_idx=80, end_idx=89, system_message=\"You only answer Yes or No to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_causal_judgement = groq_line_generate(raw_dataset = causal_judgement, output_dataset = groq_causal_judgement, start_idx=90, end_idx=99, system_message=\"You only answer Yes or No to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_causal_judgement = groq_line_generate(raw_dataset = causal_judgement, output_dataset = groq_causal_judgement, start_idx=100, end_idx=109, system_message=\"You only answer Yes or No to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_causal_judgement = groq_line_generate(raw_dataset = causal_judgement, output_dataset = groq_causal_judgement, start_idx=110, end_idx=119, system_message=\"You only answer Yes or No to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_causal_judgement = groq_line_generate(raw_dataset = causal_judgement, output_dataset = groq_causal_judgement, start_idx=120, end_idx=129, system_message=\"You only answer Yes or No to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_causal_judgement = groq_line_generate(raw_dataset = causal_judgement, output_dataset = groq_causal_judgement, start_idx=130, end_idx=139, system_message=\"You only answer Yes or No to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_causal_judgement = groq_line_generate(raw_dataset = causal_judgement, output_dataset = groq_causal_judgement, start_idx=140, end_idx=149, system_message=\"You only answer Yes or No to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_causal_judgement = groq_line_generate(raw_dataset = causal_judgement, output_dataset = groq_causal_judgement, start_idx=150, end_idx=159, system_message=\"You only answer Yes or No to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_causal_judgement = groq_line_generate(raw_dataset = causal_judgement, output_dataset = groq_causal_judgement, start_idx=160, end_idx=169, system_message=\"You only answer Yes or No to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_causal_judgement = groq_line_generate(raw_dataset = causal_judgement, output_dataset = groq_causal_judgement, start_idx=170, end_idx=179, system_message=\"You only answer Yes or No to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_causal_judgement = groq_line_generate(raw_dataset = causal_judgement, output_dataset = groq_causal_judgement, start_idx=180, end_idx=189, system_message=\"You only answer Yes or No to the following questions.\", model_name=\"llama-3.3-70b-versatile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_causal_judgement.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_csv(folder_name=\"Big Bench Hard\", df=groq_causal_judgement, file_name = \"groq_causal_judgement\", output_col = \"llama_output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Testing - disambiguation_qa.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disambiguation_qa = Big_Bench_Json_Processor_my('Big Bench Hard', 'disambiguation_qa.json').convert_df()\n",
    "disambiguation_qa.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GEMINI Applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "total_requests = 0\n",
    "successful_requests = 0\n",
    "\n",
    "\n",
    "model=genai.GenerativeModel(\n",
    "  model_name=\"gemini-1.5-flash\",\n",
    "  system_instruction=\"You only answer (A, (B), (C) or (D) to the following questions.\",\n",
    "  )\n",
    "\n",
    "disambiguation_sample = disambiguation_qa.copy()\n",
    "disambiguation_sample['gemini_output'] = None\n",
    "\n",
    "for i in range(len(disambiguation_sample)):\n",
    "    success = False\n",
    "    retries = 3\n",
    "\n",
    "    while not success and retries > 0:\n",
    "        try:\n",
    "            total_requests += 1\n",
    "\n",
    "            # Make API request\n",
    "            response = model.generate_content(disambiguation_sample['input'][i])\n",
    "            # print(response.text)\n",
    "            disambiguation_sample.loc[i, 'gemini_output'] = response.text.strip()\n",
    "            success = True\n",
    "            successful_requests += 1\n",
    "            time.sleep(5)\n",
    "\n",
    "        except Exception as e:\n",
    "            # print(f\"Error: {e}\")\n",
    "            retries -= 1\n",
    "            time.sleep(5)\n",
    "            total_requests += 1 \n",
    "\n",
    "\n",
    "\n",
    "print(f\"Total requests made: {total_requests}\")\n",
    "print(f\"Successful requests: {successful_requests}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_disambiguation = disambiguation_sample.copy()\n",
    "df_to_csv(\"Big Bench Hard\", gemini_disambiguation, file_name = \"gemini_disambiguation\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GROQ - llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disambiguation_qa['target'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disambiguation_qa.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_disambiguation_qa = disambiguation_qa.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_disambiguation_qa = groq_line_generate(raw_dataset = disambiguation_qa, output_dataset = groq_disambiguation_qa, start_idx=0, \n",
    "end_idx=9, system_message=\"You only answer Yes or No to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_disambiguation_qa = groq_line_generate(raw_dataset = disambiguation_qa, output_dataset = groq_disambiguation_qa, start_idx=10, \n",
    "end_idx=19, system_message=\"You only answer Yes or No to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_disambiguation_qa = groq_line_generate(raw_dataset = disambiguation_qa, output_dataset = groq_disambiguation_qa, start_idx=20, \n",
    "end_idx=29, system_message=\"You only answer Yes or No to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_disambiguation_qa = groq_line_generate(raw_dataset = disambiguation_qa, output_dataset = groq_disambiguation_qa, start_idx=30, \n",
    "end_idx=39, system_message=\"You only answer Yes or No to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_disambiguation_qa = groq_line_generate(raw_dataset = disambiguation_qa, output_dataset = groq_disambiguation_qa, start_idx=40, \n",
    "end_idx=49, system_message=\"You only answer Yes or No to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_disambiguation_qa = groq_line_generate(raw_dataset = disambiguation_qa, output_dataset = groq_disambiguation_qa, start_idx=50, \n",
    "end_idx=59, system_message=\"You only answer Yes or No to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_disambiguation_qa = groq_line_generate(raw_dataset = disambiguation_qa, output_dataset = groq_disambiguation_qa, start_idx=60, \n",
    "end_idx=69, system_message=\"You only answer Yes or No to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_disambiguation_qa = groq_line_generate(raw_dataset = disambiguation_qa, output_dataset = groq_disambiguation_qa, start_idx=70, \n",
    "end_idx=79, system_message=\"You only answer Yes or No to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_disambiguation_qa = groq_line_generate(raw_dataset = disambiguation_qa, output_dataset = groq_disambiguation_qa, start_idx=80, \n",
    "end_idx=89, system_message=\"You only answer Yes or No to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_disambiguation_qa = groq_line_generate(raw_dataset = disambiguation_qa, output_dataset = groq_disambiguation_qa, start_idx=90, \n",
    "end_idx=99, system_message=\"You only answer Yes or No to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_disambiguation_qa = groq_line_generate(raw_dataset = disambiguation_qa, output_dataset = groq_disambiguation_qa, start_idx=100, \n",
    "end_idx=109, system_message=\"You only answer Yes or No to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_disambiguation_qa = groq_line_generate(raw_dataset = disambiguation_qa, output_dataset = groq_disambiguation_qa, start_idx=110, \n",
    "end_idx=119, system_message=\"You only answer Yes or No to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_disambiguation_qa = groq_line_generate(raw_dataset = disambiguation_qa, output_dataset = groq_disambiguation_qa, start_idx=120, \n",
    "end_idx=129, system_message=\"You only answer Yes or No to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_disambiguation_qa = groq_line_generate(raw_dataset = disambiguation_qa, output_dataset = groq_disambiguation_qa, start_idx=130, \n",
    "end_idx=139, system_message=\"You only answer Yes or No to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_disambiguation_qa = groq_line_generate(raw_dataset = disambiguation_qa, output_dataset = groq_disambiguation_qa, start_idx=140, \n",
    "end_idx=149, system_message=\"You only answer Yes or No to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_disambiguation_qa = groq_line_generate(raw_dataset = disambiguation_qa, output_dataset = groq_disambiguation_qa, start_idx=150, \n",
    "end_idx=159, system_message=\"You only answer Yes or No to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_disambiguation_qa = groq_line_generate(raw_dataset = disambiguation_qa, output_dataset = groq_disambiguation_qa, start_idx=160, \n",
    "end_idx=169, system_message=\"You only answer Yes or No to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_disambiguation_qa = groq_line_generate(raw_dataset = disambiguation_qa, output_dataset = groq_disambiguation_qa, start_idx=170, end_idx=179, system_message=\"You only answer Yes or No to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_disambiguation_qa = groq_line_generate(raw_dataset = disambiguation_qa, output_dataset = groq_disambiguation_qa, start_idx=180, end_idx=189, system_message=\"You only answer Yes or No to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_disambiguation_qa = groq_line_generate(raw_dataset = disambiguation_qa, output_dataset = groq_disambiguation_qa, start_idx=190, end_idx=199, system_message=\"You only answer Yes or No to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_disambiguation_qa = groq_line_generate(raw_dataset = disambiguation_qa, output_dataset = groq_disambiguation_qa, start_idx=200, \n",
    "end_idx=209, system_message=\"You only answer Yes or No to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_disambiguation_qa = groq_line_generate(raw_dataset = disambiguation_qa, output_dataset = groq_disambiguation_qa, start_idx=210, \n",
    "end_idx=219, system_message=\"You only answer Yes or No to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_disambiguation_qa = groq_line_generate(raw_dataset = disambiguation_qa, output_dataset = groq_disambiguation_qa, start_idx=220, \n",
    "end_idx=229, system_message=\"You only answer Yes or No to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_disambiguation_qa = groq_line_generate(raw_dataset = disambiguation_qa, output_dataset = groq_disambiguation_qa, start_idx=230, end_idx=239, system_message=\"You only answer Yes or No to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_disambiguation_qa = groq_line_generate(raw_dataset = disambiguation_qa, output_dataset = groq_disambiguation_qa, start_idx=240, \n",
    "end_idx=250, system_message=\"You only answer Yes or No to the following questions.\", model_name=\"llama-3.3-70b-versatile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_disambiguation_qa.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_csv(\"Big Bench Hard\", groq_disambiguation_qa, file_name = \"groq_disambiguation_qa\", output_col = \"llama_output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Testing - dyck_languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dyck_languages = Big_Bench_Json_Processor_my('Big Bench Hard', 'dyck_languages.json').convert_df()\n",
    "dyck_languages.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GEMINI Applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_dyck_languages = gemini_generator(dyck_languages, \"You only answer set of [],{}, or () to the following questions.\")   \n",
    "\n",
    "df_to_csv(\"Big Bench Hard\", gemini_dyck_languages, file_name = \"gemini_dyck_languages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GROQ - llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dyck_languages.info()\n",
    "dyck_languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_dyck_languages = dyck_languages.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_dyck_languages = groq_line_generate(raw_dataset = dyck_languages, output_dataset = groq_dyck_languages, start_idx=0, end_idx=9, system_message=\"You only answer set of [],{}, or () to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_dyck_languages = groq_line_generate(raw_dataset = dyck_languages, output_dataset = groq_dyck_languages, start_idx=10, end_idx=19, system_message=\"You only answer set of [],{}, or () to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_dyck_languages = groq_line_generate(raw_dataset = dyck_languages, output_dataset = groq_dyck_languages, start_idx=20, end_idx=29, system_message=\"You only answer set of [],{}, or () to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_dyck_languages = groq_line_generate(raw_dataset = dyck_languages, output_dataset = groq_dyck_languages, start_idx=30, end_idx=39, system_message=\"You only answer set of [],{}, or () to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_dyck_languages = groq_line_generate(raw_dataset = dyck_languages, output_dataset = groq_dyck_languages, start_idx=40, end_idx=49, system_message=\"You only answer set of [],{}, or () to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_dyck_languages = groq_line_generate(raw_dataset = dyck_languages, output_dataset = groq_dyck_languages, start_idx=50, end_idx=59, system_message=\"You only answer set of [],{}, or () to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_dyck_languages = groq_line_generate(raw_dataset = dyck_languages, output_dataset = groq_dyck_languages, start_idx=60, end_idx=69, system_message=\"You only answer set of [],{}, or () to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_dyck_languages = groq_line_generate(raw_dataset = dyck_languages, output_dataset = groq_dyck_languages, start_idx=70, end_idx=79, system_message=\"You only answer set of [],{}, or () to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_dyck_languages = groq_line_generate(raw_dataset = dyck_languages, output_dataset = groq_dyck_languages, start_idx=80, end_idx=89, system_message=\"You only answer set of [],{}, or () to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_dyck_languages = groq_line_generate(raw_dataset = dyck_languages, output_dataset = groq_dyck_languages, start_idx=90, end_idx=99, system_message=\"You only answer set of [],{}, or () to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_dyck_languages = groq_line_generate(raw_dataset = dyck_languages, output_dataset = groq_dyck_languages, start_idx=100, end_idx=109, system_message=\"You only answer set of [],{}, or () to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_dyck_languages = groq_line_generate(raw_dataset = dyck_languages, output_dataset = groq_dyck_languages, start_idx=110, end_idx=119, system_message=\"You only answer set of [],{}, or () to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_dyck_languages = groq_line_generate(raw_dataset = dyck_languages, output_dataset = groq_dyck_languages, start_idx=120, end_idx=129, system_message=\"You only answer set of [],{}, or () to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_dyck_languages = groq_line_generate(raw_dataset = dyck_languages, output_dataset = groq_dyck_languages, start_idx=130, end_idx=139, system_message=\"You only answer set of [],{}, or () to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_dyck_languages = groq_line_generate(raw_dataset = dyck_languages, output_dataset = groq_dyck_languages, start_idx=140, end_idx=149, system_message=\"You only answer set of [],{}, or () to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_dyck_languages = groq_line_generate(raw_dataset = dyck_languages, output_dataset = groq_dyck_languages, start_idx=150, end_idx=159, system_message=\"You only answer set of [],{}, or () to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_dyck_languages = groq_line_generate(raw_dataset = dyck_languages, output_dataset = groq_dyck_languages, start_idx=160, end_idx=169, system_message=\"You only answer set of [],{}, or () to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_dyck_languages = groq_line_generate(raw_dataset = dyck_languages, output_dataset = groq_dyck_languages, start_idx=170, end_idx=179, system_message=\"You only answer set of [],{}, or () to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_dyck_languages = groq_line_generate(raw_dataset = dyck_languages, output_dataset = groq_dyck_languages, start_idx=180, end_idx=189, system_message=\"You only answer set of [],{}, or () to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_dyck_languages = groq_line_generate(raw_dataset = dyck_languages, output_dataset = groq_dyck_languages, start_idx=190, end_idx=199, system_message=\"You only answer set of [],{}, or () to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_dyck_languages = groq_line_generate(raw_dataset = dyck_languages, output_dataset = groq_dyck_languages, start_idx=200, end_idx=209, system_message=\"You only answer set of [],{}, or () to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_dyck_languages = groq_line_generate(raw_dataset = dyck_languages, output_dataset = groq_dyck_languages, start_idx=210, end_idx=219, system_message=\"You only answer set of [],{}, or () to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_dyck_languages = groq_line_generate(raw_dataset = dyck_languages, output_dataset = groq_dyck_languages, start_idx=220, end_idx=229, system_message=\"You only answer set of [],{}, or () to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_dyck_languages = groq_line_generate(raw_dataset = dyck_languages, output_dataset = groq_dyck_languages, start_idx=230, end_idx=239, system_message=\"You only answer set of [],{}, or () to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_dyck_languages = groq_line_generate(raw_dataset = dyck_languages, output_dataset = groq_dyck_languages, start_idx=240, end_idx=250, system_message=\"You only answer set of [],{}, or () to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_dyck_languages.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_csv(\"Big Bench Hard\", groq_dyck_languages, file_name = \"groq_dyck_languages\", output_col = \"llama_output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Testing - formal_fallacies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formal_fallacies = Big_Bench_Json_Processor_my('Big Bench Hard', 'formal_fallacies.json').convert_df()\n",
    "formal_fallacies.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GEMINI Applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_formal_fallacies = gemini_generator(formal_fallacies, \"You only answer invalid or valid to the following questions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_csv(\"Big Bench Hard\", gemini_dyck_languages, file_name = \"gemini_formal_fallacies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GROQ - llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_formal_fallacies = formal_fallacies.copy()\n",
    "groq_formal_fallacies.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_formal_fallacies = groq_line_generate(raw_dataset = formal_fallacies, output_dataset = groq_formal_fallacies, start_idx=0, end_idx=9, system_message=\"You only answer invalid or valid to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_formal_fallacies = groq_line_generate(raw_dataset = formal_fallacies, output_dataset = groq_formal_fallacies, start_idx=10, end_idx=19, system_message=\"You only answer invalid or valid to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_formal_fallacies = groq_line_generate(raw_dataset = formal_fallacies, output_dataset = groq_formal_fallacies, start_idx=20, end_idx=29, system_message=\"You only answer invalid or valid to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_formal_fallacies = groq_line_generate(raw_dataset = formal_fallacies, output_dataset = groq_formal_fallacies, start_idx=30, end_idx=39, system_message=\"You only answer invalid or valid to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_formal_fallacies = groq_line_generate(raw_dataset = formal_fallacies, output_dataset = groq_formal_fallacies, start_idx=40, end_idx=49, system_message=\"You only answer invalid or valid to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_formal_fallacies = groq_line_generate(raw_dataset = formal_fallacies, output_dataset = groq_formal_fallacies, start_idx=50, end_idx=59, system_message=\"You only answer invalid or valid to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_formal_fallacies = groq_line_generate(raw_dataset = formal_fallacies, output_dataset = groq_formal_fallacies, start_idx=60, end_idx=69, system_message=\"You only answer invalid or valid to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_formal_fallacies = groq_line_generate(raw_dataset = formal_fallacies, output_dataset = groq_formal_fallacies, start_idx=70, end_idx=79, system_message=\"You only answer invalid or valid to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_formal_fallacies = groq_line_generate(raw_dataset = formal_fallacies, output_dataset = groq_formal_fallacies, start_idx=80, end_idx=89, system_message=\"You only answer invalid or valid to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_formal_fallacies = groq_line_generate(raw_dataset = formal_fallacies, output_dataset = groq_formal_fallacies, start_idx=90, end_idx=99, system_message=\"You only answer invalid or valid to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_formal_fallacies = groq_line_generate(raw_dataset = formal_fallacies, output_dataset = groq_formal_fallacies, start_idx=100, end_idx=109, system_message=\"You only answer invalid or valid to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_formal_fallacies = groq_line_generate(raw_dataset = formal_fallacies, output_dataset = groq_formal_fallacies, start_idx=110, end_idx=119, system_message=\"You only answer invalid or valid to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_formal_fallacies = groq_line_generate(raw_dataset = formal_fallacies, output_dataset = groq_formal_fallacies, start_idx=120, end_idx=129, system_message=\"You only answer invalid or valid to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_formal_fallacies = groq_line_generate(raw_dataset = formal_fallacies, output_dataset = groq_formal_fallacies, start_idx=130, end_idx=139, system_message=\"You only answer invalid or valid to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_formal_fallacies = groq_line_generate(raw_dataset = formal_fallacies, output_dataset = groq_formal_fallacies, start_idx=140, end_idx=149, system_message=\"You only answer invalid or valid to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_formal_fallacies = groq_line_generate(raw_dataset = formal_fallacies, output_dataset = groq_formal_fallacies, start_idx=150, end_idx=159, system_message=\"You only answer invalid or valid to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_formal_fallacies = groq_line_generate(raw_dataset = formal_fallacies, output_dataset = groq_formal_fallacies, start_idx=160, end_idx=169, system_message=\"You only answer invalid or valid to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_formal_fallacies = groq_line_generate(raw_dataset = formal_fallacies, output_dataset = groq_formal_fallacies, start_idx=170, end_idx=179, system_message=\"You only answer invalid or valid to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_formal_fallacies = groq_line_generate(raw_dataset = formal_fallacies, output_dataset = groq_formal_fallacies, start_idx=180, end_idx=189, system_message=\"You only answer invalid or valid to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_formal_fallacies = groq_line_generate(raw_dataset = formal_fallacies, output_dataset = groq_formal_fallacies, start_idx=190, end_idx=199, system_message=\"You only answer invalid or valid to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_formal_fallacies = groq_line_generate(raw_dataset = formal_fallacies, output_dataset = groq_formal_fallacies, start_idx=200, end_idx=209, system_message=\"You only answer invalid or valid to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_formal_fallacies = groq_line_generate(raw_dataset = formal_fallacies, output_dataset = groq_formal_fallacies, start_idx=210, end_idx=219, system_message=\"You only answer invalid or valid to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_formal_fallacies = groq_line_generate(raw_dataset = formal_fallacies, output_dataset = groq_formal_fallacies, start_idx=220, end_idx=229, system_message=\"You only answer invalid or valid to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_formal_fallacies = groq_line_generate(raw_dataset = formal_fallacies, output_dataset = groq_formal_fallacies, start_idx=230, end_idx=239, system_message=\"You only answer invalid or valid to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_formal_fallacies = groq_line_generate(raw_dataset = formal_fallacies, output_dataset = groq_formal_fallacies, start_idx=240, end_idx=250, system_message=\"You only answer invalid or valid to the following questions.\", model_name=\"llama-3.3-70b-versatile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_formal_fallacies.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Need to handle the empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_csv(\"Big Bench Hard\", groq_formal_fallacies, file_name = \"groq_formal_fallacies\", output_col = \"llama_output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Testing - geometric_shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geometric_shapes = Big_Bench_Json_Processor_my('Big Bench Hard', 'geometric_shapes.json').convert_df()\n",
    "geometric_shapes.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geometric_shapes['target'] .unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geometric_shapes.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GEMINI Applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_geomnetric_shapes = gemini_generator(geometric_shapes, \"You only answer A, B, C, D, E, F, J, G, H, K to the following questions.\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_csv(\"Big Bench Hard\", gemini_geomnetric_shapes, file_name = \"gemini_geomnetric_shapes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GROQ - llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_geomnetric_shapes = geometric_shapes.copy()\n",
    "groq_geomnetric_shapes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total requests made: 10\n",
      "Successful requests: 10\n",
      "Total requests made: 10\n",
      "Successful requests: 10\n",
      "Total requests made: 10\n",
      "Successful requests: 10\n",
      "Total requests made: 10\n",
      "Successful requests: 10\n",
      "Total requests made: 10\n",
      "Successful requests: 10\n",
      "Total requests made: 10\n",
      "Successful requests: 10\n",
      "Total requests made: 10\n",
      "Successful requests: 10\n",
      "Total requests made: 10\n",
      "Successful requests: 10\n",
      "Total requests made: 10\n",
      "Successful requests: 10\n",
      "Total requests made: 10\n",
      "Successful requests: 10\n",
      "Total requests made: 10\n",
      "Successful requests: 10\n",
      "Total requests made: 10\n",
      "Successful requests: 10\n",
      "Total requests made: 10\n",
      "Successful requests: 10\n",
      "Total requests made: 10\n",
      "Successful requests: 10\n",
      "Total requests made: 10\n",
      "Successful requests: 10\n",
      "Total requests made: 10\n",
      "Successful requests: 10\n",
      "Total requests made: 10\n",
      "Successful requests: 10\n",
      "Total requests made: 10\n",
      "Successful requests: 10\n",
      "Total requests made: 10\n",
      "Successful requests: 10\n",
      "Total requests made: 10\n",
      "Successful requests: 10\n"
     ]
    }
   ],
   "source": [
    "groq_geomnetric_shapes = groq_line_generate(raw_dataset = geometric_shapes, output_dataset = groq_geomnetric_shapes, start_idx=0, end_idx=9, system_message=\"You only answer A, B, C, D, E, F, J, G, H, K to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_geomnetric_shapes = groq_line_generate(raw_dataset = geometric_shapes, output_dataset = groq_geomnetric_shapes, start_idx=10, end_idx=19, system_message=\"You only answer A, B, C, D, E, F, J, G, H, K to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_geomnetric_shapes = groq_line_generate(raw_dataset = geometric_shapes, output_dataset = groq_geomnetric_shapes, start_idx=20, end_idx=29, system_message=\"You only answer A, B, C, D, E, F, J, G, H, K to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_geomnetric_shapes = groq_line_generate(raw_dataset = geometric_shapes, output_dataset = groq_geomnetric_shapes, start_idx=30, end_idx=39, system_message=\"You only answer A, B, C, D, E, F, J, G, H, K to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_geomnetric_shapes = groq_line_generate(raw_dataset = geometric_shapes, output_dataset = groq_geomnetric_shapes, start_idx=40, end_idx=49, system_message=\"You only answer A, B, C, D, E, F, J, G, H, K to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_geomnetric_shapes = groq_line_generate(raw_dataset = geometric_shapes, output_dataset = groq_geomnetric_shapes, start_idx=50, end_idx=59, system_message=\"You only answer A, B, C, D, E, F, J, G, H, K to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_geomnetric_shapes = groq_line_generate(raw_dataset = geometric_shapes, output_dataset = groq_geomnetric_shapes, start_idx=60, end_idx=69, system_message=\"You only answer A, B, C, D, E, F, J, G, H, K to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_geomnetric_shapes = groq_line_generate(raw_dataset = geometric_shapes, output_dataset = groq_geomnetric_shapes, start_idx=70, end_idx=79, system_message=\"You only answer A, B, C, D, E, F, J, G, H, K to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_geomnetric_shapes = groq_line_generate(raw_dataset = geometric_shapes, output_dataset = groq_geomnetric_shapes, start_idx=80, end_idx=89, system_message=\"You only answer A, B, C, D, E, F, J, G, H, K to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_geomnetric_shapes = groq_line_generate(raw_dataset = geometric_shapes, output_dataset = groq_geomnetric_shapes, start_idx=90, end_idx=99, system_message=\"You only answer A, B, C, D, E, F, J, G, H, K to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_geomnetric_shapes = groq_line_generate(raw_dataset = geometric_shapes, output_dataset = groq_geomnetric_shapes, start_idx=100, end_idx=109, system_message=\"You only answer A, B, C, D, E, F, J, G, H, K to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_geomnetric_shapes = groq_line_generate(raw_dataset = geometric_shapes, output_dataset = groq_geomnetric_shapes, start_idx=110, end_idx=119, system_message=\"You only answer A, B, C, D, E, F, J, G, H, K to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_geomnetric_shapes = groq_line_generate(raw_dataset = geometric_shapes, output_dataset = groq_geomnetric_shapes, start_idx=120, end_idx=129, system_message=\"You only answer A, B, C, D, E, F, J, G, H, K to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_geomnetric_shapes = groq_line_generate(raw_dataset = geometric_shapes, output_dataset = groq_geomnetric_shapes, start_idx=130, end_idx=139, system_message=\"You only answer A, B, C, D, E, F, J, G, H, K to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_geomnetric_shapes = groq_line_generate(raw_dataset = geometric_shapes, output_dataset = groq_geomnetric_shapes, start_idx=140, end_idx=149, system_message=\"You only answer A, B, C, D, E, F, J, G, H, K to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_geomnetric_shapes = groq_line_generate(raw_dataset = geometric_shapes, output_dataset = groq_geomnetric_shapes, start_idx=150, end_idx=159, system_message=\"You only answer A, B, C, D, E, F, J, G, H, K to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_geomnetric_shapes = groq_line_generate(raw_dataset = geometric_shapes, output_dataset = groq_geomnetric_shapes, start_idx=160, end_idx=169, system_message=\"You only answer A, B, C, D, E, F, J, G, H, K to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_geomnetric_shapes = groq_line_generate(raw_dataset = geometric_shapes, output_dataset = groq_geomnetric_shapes, start_idx=170, end_idx=179, system_message=\"You only answer A, B, C, D, E, F, J, G, H, K to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_geomnetric_shapes = groq_line_generate(raw_dataset = geometric_shapes, output_dataset = groq_geomnetric_shapes, start_idx=180, end_idx=189, system_message=\"You only answer A, B, C, D, E, F, J, G, H, K to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_geomnetric_shapes = groq_line_generate(raw_dataset = geometric_shapes, output_dataset = groq_geomnetric_shapes, start_idx=190, end_idx=199, system_message=\"You only answer A, B, C, D, E, F, J, G, H, K to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_geomnetric_shapes = groq_line_generate(raw_dataset = geometric_shapes, output_dataset = groq_geomnetric_shapes, start_idx=200, end_idx=209, system_message=\"You only answer A, B, C, D, E, F, J, G, H, K to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_geomnetric_shapes = groq_line_generate(raw_dataset = geometric_shapes, output_dataset = groq_geomnetric_shapes, start_idx=210, end_idx=219, system_message=\"You only answer A, B, C, D, E, F, J, G, H, K to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_geomnetric_shapes = groq_line_generate(raw_dataset = geometric_shapes, output_dataset = groq_geomnetric_shapes, start_idx=220, end_idx=229, system_message=\"You only answer A, B, C, D, E, F, J, G, H, K to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_geomnetric_shapes = groq_line_generate(raw_dataset = geometric_shapes, output_dataset = groq_geomnetric_shapes, start_idx=230, end_idx=239, system_message=\"You only answer A, B, C, D, E, F, J, G, H, K to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_geomnetric_shapes = groq_line_generate(raw_dataset = geometric_shapes, output_dataset = groq_geomnetric_shapes, start_idx=240, end_idx=249, system_message=\"You only answer A, B, C, D, E, F, J, G, H, K to the following questions.\", model_name=\"llama-3.3-70b-versatile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 250 entries, 0 to 249\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   input         250 non-null    object\n",
      " 1   target        250 non-null    object\n",
      " 2   llama_output  250 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 6.0+ KB\n"
     ]
    }
   ],
   "source": [
    "groq_geomnetric_shapes.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame exported successfully to /Users/ezishr/Library/CloudStorage/OneDrive-UniversityofCincinnati/Undergraduate Research/Check points/Big Bench Hard/groq_geomnetric_shapes.csv!\n"
     ]
    }
   ],
   "source": [
    "df_to_csv(\"Big Bench Hard\", groq_geomnetric_shapes, file_name = \"groq_geomnetric_shapes\", output_col = \"llama_output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Testing - hyperbaton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 250 entries, 0 to 249\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   input   250 non-null    object\n",
      " 1   target  250 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 4.0+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Which sentence has the correct adjective order:\\nOptions:\\n(A) midsize old grey Brazilian sweater\\n(B) midsize grey Brazilian old sweater</td>\n",
       "      <td>(A)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which sentence has the correct adjective order:\\nOptions:\\n(A) rubber gray walking Mexican midsize cat\\n(B) midsize gray Mexican rubber walking cat</td>\n",
       "      <td>(B)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Which sentence has the correct adjective order:\\nOptions:\\n(A) mysterious big Indian iron smoking motorcycle\\n(B) big smoking mysterious Indian iron motorcycle</td>\n",
       "      <td>(A)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Which sentence has the correct adjective order:\\nOptions:\\n(A) cloth smoking rectangular motorcycle\\n(B) rectangular cloth smoking motorcycle</td>\n",
       "      <td>(B)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which sentence has the correct adjective order:\\nOptions:\\n(A) pyramidal American glass exercise surfboard\\n(B) glass exercise American pyramidal surfboard</td>\n",
       "      <td>(A)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                             input  \\\n",
       "0                        Which sentence has the correct adjective order:\\nOptions:\\n(A) midsize old grey Brazilian sweater\\n(B) midsize grey Brazilian old sweater   \n",
       "1              Which sentence has the correct adjective order:\\nOptions:\\n(A) rubber gray walking Mexican midsize cat\\n(B) midsize gray Mexican rubber walking cat   \n",
       "2  Which sentence has the correct adjective order:\\nOptions:\\n(A) mysterious big Indian iron smoking motorcycle\\n(B) big smoking mysterious Indian iron motorcycle   \n",
       "3                    Which sentence has the correct adjective order:\\nOptions:\\n(A) cloth smoking rectangular motorcycle\\n(B) rectangular cloth smoking motorcycle   \n",
       "4      Which sentence has the correct adjective order:\\nOptions:\\n(A) pyramidal American glass exercise surfboard\\n(B) glass exercise American pyramidal surfboard   \n",
       "\n",
       "  target  \n",
       "0    (A)  \n",
       "1    (B)  \n",
       "2    (A)  \n",
       "3    (B)  \n",
       "4    (A)  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperbaton = Big_Bench_Json_Processor_my('Big Bench Hard', 'hyperbaton.json').convert_df()\n",
    "hyperbaton.info()\n",
    "hyperbaton.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 250 entries, 0 to 249\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   input   250 non-null    object\n",
      " 1   target  250 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 4.0+ KB\n"
     ]
    }
   ],
   "source": [
    "hyperbaton.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GEMINI Applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_hyperbaton = gemini_generator(hyperbaton, \"You only answer A or B to the following questions.\")\n",
    "df_to_csv(\"Big Bench Hard\", gemini_hyperbaton, file_name = \"gemini_hyperbaton\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GROQ - llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_hyperbaton = hyperbaton.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['(A)', '(B)'], dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groq_hyperbaton['target'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total requests made: 10\n",
      "Successful requests: 10\n",
      "Total requests made: 10\n",
      "Successful requests: 10\n",
      "Total requests made: 10\n",
      "Successful requests: 10\n",
      "Total requests made: 10\n",
      "Successful requests: 10\n",
      "Total requests made: 10\n",
      "Successful requests: 10\n",
      "Total requests made: 10\n",
      "Successful requests: 10\n",
      "Total requests made: 10\n",
      "Successful requests: 10\n",
      "Total requests made: 10\n",
      "Successful requests: 10\n",
      "Total requests made: 10\n",
      "Successful requests: 10\n",
      "Total requests made: 10\n",
      "Successful requests: 10\n",
      "Total requests made: 10\n",
      "Successful requests: 10\n",
      "Total requests made: 10\n",
      "Successful requests: 10\n",
      "Total requests made: 10\n",
      "Successful requests: 10\n",
      "Total requests made: 10\n",
      "Successful requests: 10\n",
      "Total requests made: 10\n",
      "Successful requests: 10\n",
      "Total requests made: 10\n",
      "Successful requests: 10\n",
      "Total requests made: 10\n",
      "Successful requests: 10\n",
      "Total requests made: 10\n",
      "Successful requests: 10\n",
      "Total requests made: 10\n",
      "Successful requests: 10\n",
      "Total requests made: 10\n",
      "Successful requests: 10\n",
      "Total requests made: 10\n",
      "Successful requests: 10\n",
      "Total requests made: 10\n",
      "Successful requests: 10\n",
      "Total requests made: 10\n",
      "Successful requests: 10\n",
      "Total requests made: 10\n",
      "Successful requests: 10\n",
      "Total requests made: 10\n",
      "Successful requests: 10\n"
     ]
    }
   ],
   "source": [
    "groq_hyperbaton = groq_line_generate(raw_dataset = hyperbaton, output_dataset = groq_hyperbaton, start_idx=0, end_idx=9, system_message=\"You only answer A, B to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_hyperbaton = groq_line_generate(raw_dataset = hyperbaton, output_dataset = groq_hyperbaton, start_idx=10, end_idx=19, system_message=\"You only answer A, B to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_hyperbaton = groq_line_generate(raw_dataset = hyperbaton, output_dataset = groq_hyperbaton, start_idx=20, end_idx=29, system_message=\"You only answer A, B to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_hyperbaton = groq_line_generate(raw_dataset = hyperbaton, output_dataset = groq_hyperbaton, start_idx=30, end_idx=39, system_message=\"You only answer A, B to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_hyperbaton = groq_line_generate(raw_dataset = hyperbaton, output_dataset = groq_hyperbaton, start_idx=40, end_idx=49, system_message=\"You only answer A, B to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_hyperbaton = groq_line_generate(raw_dataset = hyperbaton, output_dataset = groq_hyperbaton, start_idx=50, end_idx=59, system_message=\"You only answer A, B to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_hyperbaton = groq_line_generate(raw_dataset = hyperbaton, output_dataset = groq_hyperbaton, start_idx=60, end_idx=69, system_message=\"You only answer A, B to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_hyperbaton = groq_line_generate(raw_dataset = hyperbaton, output_dataset = groq_hyperbaton, start_idx=70, end_idx=79, system_message=\"You only answer A, B to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_hyperbaton = groq_line_generate(raw_dataset = hyperbaton, output_dataset = groq_hyperbaton, start_idx=80, end_idx=89, system_message=\"You only answer A, B to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_hyperbaton = groq_line_generate(raw_dataset = hyperbaton, output_dataset = groq_hyperbaton, start_idx=90, end_idx=99, system_message=\"You only answer A, B to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_hyperbaton = groq_line_generate(raw_dataset = hyperbaton, output_dataset = groq_hyperbaton, start_idx=100, end_idx=109, system_message=\"You only answer A, B to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_hyperbaton = groq_line_generate(raw_dataset = hyperbaton, output_dataset = groq_hyperbaton, start_idx=110, end_idx=119, system_message=\"You only answer A, B to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_hyperbaton = groq_line_generate(raw_dataset = hyperbaton, output_dataset = groq_hyperbaton, start_idx=120, end_idx=129, system_message=\"You only answer A, B to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_hyperbaton = groq_line_generate(raw_dataset = hyperbaton, output_dataset = groq_hyperbaton, start_idx=130, end_idx=139, system_message=\"You only answer A, B to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_hyperbaton = groq_line_generate(raw_dataset = hyperbaton, output_dataset = groq_hyperbaton, start_idx=140, end_idx=149, system_message=\"You only answer A, B to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_hyperbaton = groq_line_generate(raw_dataset = hyperbaton, output_dataset = groq_hyperbaton, start_idx=150, end_idx=159, system_message=\"You only answer A, B to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_hyperbaton = groq_line_generate(raw_dataset = hyperbaton, output_dataset = groq_hyperbaton, start_idx=160, end_idx=169, system_message=\"You only answer A, B to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_hyperbaton = groq_line_generate(raw_dataset = hyperbaton, output_dataset = groq_hyperbaton, start_idx=170, end_idx=179, system_message=\"You only answer A, B to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_hyperbaton = groq_line_generate(raw_dataset = hyperbaton, output_dataset = groq_hyperbaton, start_idx=180, end_idx=189, system_message=\"You only answer A, B to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_hyperbaton = groq_line_generate(raw_dataset = hyperbaton, output_dataset = groq_hyperbaton, start_idx=190, end_idx=199, system_message=\"You only answer A, B to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_hyperbaton = groq_line_generate(raw_dataset = hyperbaton, output_dataset = groq_hyperbaton, start_idx=200, end_idx=209, system_message=\"You only answer A, B to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_hyperbaton = groq_line_generate(raw_dataset = hyperbaton, output_dataset = groq_hyperbaton, start_idx=210, end_idx=219, system_message=\"You only answer A, B to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_hyperbaton = groq_line_generate(raw_dataset = hyperbaton, output_dataset = groq_hyperbaton, start_idx=220, end_idx=229, system_message=\"You only answer A, B to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_hyperbaton = groq_line_generate(raw_dataset = hyperbaton, output_dataset = groq_hyperbaton, start_idx=230, end_idx=239, system_message=\"You only answer A, B to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_hyperbaton = groq_line_generate(raw_dataset = hyperbaton, output_dataset = groq_hyperbaton, start_idx=240, end_idx=249, system_message=\"You only answer A, B to the following questions.\", model_name=\"llama-3.3-70b-versatile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 250 entries, 0 to 249\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   input         250 non-null    object\n",
      " 1   target        250 non-null    object\n",
      " 2   llama_output  250 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 6.0+ KB\n",
      "DataFrame exported successfully to /Users/ezishr/Library/CloudStorage/OneDrive-UniversityofCincinnati/Undergraduate Research/Check points/Big Bench Hard/groq_hyperbaton.csv!\n"
     ]
    }
   ],
   "source": [
    "groq_hyperbaton.info()\n",
    "df_to_csv(\"Big Bench Hard\", groq_hyperbaton, file_name = \"groq_hyperbaton\", output_col = \"llama_output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Testing - logical_deduction_five_objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "logical_deduction_five = Big_Bench_Json_Processor_my('Big Bench Hard', 'logical_deduction_five_objects.json').convert_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a branch, there are five birds: a quail, an owl, a raven, a falcon, and a robin. The owl is the leftmost. The robin is to the left of the raven. The quail is the rightmost. The raven is the third from the left.\\nOptions:\\n(A) The quail is the rightmost\\n(B) The owl is the rightmost\\n(C) The raven is the rightmost\\n(D) The falcon is the rightmost\\n(E) The robin is the rightmost</td>\n",
       "      <td>(A)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph. In an antique car show, there are five vehicles: a hatchback, a bus, a convertible, a tractor, and a minivan. The tractor is older than the bus. The minivan is newer than the bus. The hatchback is the second-newest. The minivan is older than the convertible.\\nOptions:\\n(A) The hatchback is the second-oldest\\n(B) The bus is the second-oldest\\n(C) The convertible is the second-oldest\\n(D) The tractor is the second-oldest\\n(E) The minivan is the second-oldest</td>\n",
       "      <td>(B)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a shelf, there are five books: a yellow book, a green book, a gray book, a blue book, and an orange book. The gray book is to the left of the green book. The gray book is the second from the right. The yellow book is to the right of the orange book. The blue book is the second from the left.\\nOptions:\\n(A) The yellow book is the leftmost\\n(B) The green book is the leftmost\\n(C) The gray book is the leftmost\\n(D) The blue book is the leftmost\\n(E) The orange book is the leftmost</td>\n",
       "      <td>(E)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a branch, there are five birds: a raven, an owl, a cardinal, a hummingbird, and a robin. The cardinal is the rightmost. The raven is to the left of the owl. The robin is to the right of the hummingbird. The hummingbird is the third from the left.\\nOptions:\\n(A) The raven is the second from the left\\n(B) The owl is the second from the left\\n(C) The cardinal is the second from the left\\n(D) The hummingbird is the second from the left\\n(E) The robin is the second from the left</td>\n",
       "      <td>(B)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a shelf, there are five books: a green book, an orange book, a gray book, a yellow book, and a purple book. The green book is the rightmost. The gray book is to the right of the orange book. The purple book is to the left of the yellow book. The purple book is to the right of the gray book.\\nOptions:\\n(A) The green book is the second from the right\\n(B) The orange book is the second from the right\\n(C) The gray book is the second from the right\\n(D) The yellow book is the second from the right\\n(E) The purple book is the second from the right</td>\n",
       "      <td>(D)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            input  \\\n",
       "0                                                                                                                                                                           The following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a branch, there are five birds: a quail, an owl, a raven, a falcon, and a robin. The owl is the leftmost. The robin is to the left of the raven. The quail is the rightmost. The raven is the third from the left.\\nOptions:\\n(A) The quail is the rightmost\\n(B) The owl is the rightmost\\n(C) The raven is the rightmost\\n(D) The falcon is the rightmost\\n(E) The robin is the rightmost   \n",
       "1                                                                                             The following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph. In an antique car show, there are five vehicles: a hatchback, a bus, a convertible, a tractor, and a minivan. The tractor is older than the bus. The minivan is newer than the bus. The hatchback is the second-newest. The minivan is older than the convertible.\\nOptions:\\n(A) The hatchback is the second-oldest\\n(B) The bus is the second-oldest\\n(C) The convertible is the second-oldest\\n(D) The tractor is the second-oldest\\n(E) The minivan is the second-oldest   \n",
       "2                                                                    The following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a shelf, there are five books: a yellow book, a green book, a gray book, a blue book, and an orange book. The gray book is to the left of the green book. The gray book is the second from the right. The yellow book is to the right of the orange book. The blue book is the second from the left.\\nOptions:\\n(A) The yellow book is the leftmost\\n(B) The green book is the leftmost\\n(C) The gray book is the leftmost\\n(D) The blue book is the leftmost\\n(E) The orange book is the leftmost   \n",
       "3                                                                        The following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a branch, there are five birds: a raven, an owl, a cardinal, a hummingbird, and a robin. The cardinal is the rightmost. The raven is to the left of the owl. The robin is to the right of the hummingbird. The hummingbird is the third from the left.\\nOptions:\\n(A) The raven is the second from the left\\n(B) The owl is the second from the left\\n(C) The cardinal is the second from the left\\n(D) The hummingbird is the second from the left\\n(E) The robin is the second from the left   \n",
       "4  The following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a shelf, there are five books: a green book, an orange book, a gray book, a yellow book, and a purple book. The green book is the rightmost. The gray book is to the right of the orange book. The purple book is to the left of the yellow book. The purple book is to the right of the gray book.\\nOptions:\\n(A) The green book is the second from the right\\n(B) The orange book is the second from the right\\n(C) The gray book is the second from the right\\n(D) The yellow book is the second from the right\\n(E) The purple book is the second from the right   \n",
       "\n",
       "  target  \n",
       "0    (A)  \n",
       "1    (B)  \n",
       "2    (E)  \n",
       "3    (B)  \n",
       "4    (D)  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logical_deduction_five.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['(A)', '(B)', '(E)', '(D)', '(C)'], dtype=object)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logical_deduction_five['target'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GEMINI Applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_logical_deduction_five = gemini_generator(logical_deduction_five, \"You only answer A, B, C, D, E to the following questions.\")\n",
    "df_to_csv(\"Big Bench Hard\", gemini_logical_deduction_five, file_name = \"gemini_logical_deduction_five\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GROQ - llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_logical_five = logical_deduction_five.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total requests made: 10\n",
      "Successful requests: 10\n",
      "Total requests made: 10\n",
      "Successful requests: 10\n",
      "Total requests made: 10\n",
      "Successful requests: 10\n",
      "Total requests made: 10\n",
      "Successful requests: 10\n",
      "Total requests made: 16\n",
      "Successful requests: 10\n",
      "Total requests made: 12\n",
      "Successful requests: 10\n",
      "Total requests made: 19\n",
      "Successful requests: 9\n",
      "Total requests made: 26\n",
      "Successful requests: 10\n",
      "Total requests made: 16\n",
      "Successful requests: 10\n",
      "Total requests made: 19\n",
      "Successful requests: 9\n",
      "Total requests made: 20\n",
      "Successful requests: 10\n",
      "Total requests made: 36\n",
      "Successful requests: 6\n",
      "Total requests made: 45\n",
      "Successful requests: 5\n",
      "Total requests made: 36\n",
      "Successful requests: 8\n",
      "Total requests made: 42\n",
      "Successful requests: 4\n",
      "Total requests made: 45\n",
      "Successful requests: 5\n",
      "Total requests made: 50\n",
      "Successful requests: 2\n",
      "Total requests made: 37\n",
      "Successful requests: 7\n",
      "Total requests made: 14\n",
      "Successful requests: 10\n",
      "Total requests made: 16\n",
      "Successful requests: 10\n",
      "Total requests made: 31\n",
      "Successful requests: 7\n",
      "Total requests made: 44\n",
      "Successful requests: 6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 45\u001b[0m\n\u001b[1;32m     41\u001b[0m groq_logical_five \u001b[38;5;241m=\u001b[39m groq_line_generate(raw_dataset \u001b[38;5;241m=\u001b[39m logical_deduction_five, output_dataset \u001b[38;5;241m=\u001b[39m groq_logical_five, start_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, end_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m209\u001b[39m, system_message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou only answer A, B to the following questions.\u001b[39m\u001b[38;5;124m\"\u001b[39m, model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama-3.3-70b-versatile\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m groq_logical_five \u001b[38;5;241m=\u001b[39m groq_line_generate(raw_dataset \u001b[38;5;241m=\u001b[39m logical_deduction_five, output_dataset \u001b[38;5;241m=\u001b[39m groq_logical_five, start_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m210\u001b[39m, end_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m219\u001b[39m, system_message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou only answer A, B to the following questions.\u001b[39m\u001b[38;5;124m\"\u001b[39m, model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama-3.3-70b-versatile\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 45\u001b[0m groq_logical_five \u001b[38;5;241m=\u001b[39m \u001b[43mgroq_line_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_dataset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlogical_deduction_five\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dataset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgroq_logical_five\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m220\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m229\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_message\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou only answer A, B to the following questions.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mllama-3.3-70b-versatile\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m groq_logical_five \u001b[38;5;241m=\u001b[39m groq_line_generate(raw_dataset \u001b[38;5;241m=\u001b[39m logical_deduction_five, output_dataset \u001b[38;5;241m=\u001b[39m groq_logical_five, start_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m230\u001b[39m, end_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m239\u001b[39m, system_message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou only answer A, B to the following questions.\u001b[39m\u001b[38;5;124m\"\u001b[39m, model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama-3.3-70b-versatile\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     49\u001b[0m groq_logical_five \u001b[38;5;241m=\u001b[39m groq_line_generate(raw_dataset \u001b[38;5;241m=\u001b[39m logical_deduction_five, output_dataset \u001b[38;5;241m=\u001b[39m groq_logical_five, start_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m240\u001b[39m, end_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m249\u001b[39m, system_message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou only answer A, B to the following questions.\u001b[39m\u001b[38;5;124m\"\u001b[39m, model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama-3.3-70b-versatile\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 11\u001b[0m, in \u001b[0;36mgroq_line_generate\u001b[0;34m(raw_dataset, output_dataset, start_idx, end_idx, system_message, model_name)\u001b[0m\n\u001b[1;32m      9\u001b[0m sample \u001b[38;5;241m=\u001b[39m raw_dataset\u001b[38;5;241m.\u001b[39mloc[start_idx:end_idx, ]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     10\u001b[0m sample\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 11\u001b[0m groq_sample \u001b[38;5;241m=\u001b[39m \u001b[43mgroq\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_message\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m output_dataset\u001b[38;5;241m.\u001b[39mloc[start_idx:end_idx, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama_output\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m groq_sample[model_name]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output_dataset\n",
      "Cell \u001b[0;32mIn[4], line 30\u001b[0m, in \u001b[0;36mgroq\u001b[0;34m(data, system_message, model_name)\u001b[0m\n\u001b[1;32m     27\u001b[0m total_requests \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Make a request to the GROQ API\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m chat_completion \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_message\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m response \u001b[38;5;241m=\u001b[39m chat_completion\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m     46\u001b[0m sample\u001b[38;5;241m.\u001b[39mloc[i, model_name] \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[0;32m~/anaconda3/envs/python_env/lib/python3.12/site-packages/groq/resources/chat/completions.py:298\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    186\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    187\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    188\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;124;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/openai/v1/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python_env/lib/python3.12/site-packages/groq/_base_client.py:1263\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1250\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1251\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1258\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1259\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1260\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1261\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1262\u001b[0m     )\n\u001b[0;32m-> 1263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/python_env/lib/python3.12/site-packages/groq/_base_client.py:955\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    953\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 955\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python_env/lib/python3.12/site-packages/groq/_base_client.py:991\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m    988\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSending HTTP Request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, request\u001b[38;5;241m.\u001b[39mmethod, request\u001b[38;5;241m.\u001b[39murl)\n\u001b[1;32m    990\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 991\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    993\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    994\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    995\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    996\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    997\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/python_env/lib/python3.12/site-packages/httpx/_client.py:926\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[1;32m    924\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 926\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/anaconda3/envs/python_env/lib/python3.12/site-packages/httpx/_client.py:954\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    951\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 954\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    959\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    960\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/python_env/lib/python3.12/site-packages/httpx/_client.py:991\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    989\u001b[0m     hook(request)\n\u001b[0;32m--> 991\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    993\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/anaconda3/envs/python_env/lib/python3.12/site-packages/httpx/_client.py:1027\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1023\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1024\u001b[0m     )\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1027\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1031\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m~/anaconda3/envs/python_env/lib/python3.12/site-packages/httpx/_transports/default.py:236\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    223\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    224\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    225\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    233\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    234\u001b[0m )\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 236\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    241\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    242\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    243\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    244\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    245\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/python_env/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:216\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    213\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, Iterable)\n",
      "File \u001b[0;32m~/anaconda3/envs/python_env/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:196\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    192\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m~/anaconda3/envs/python_env/lib/python3.12/site-packages/httpcore/_sync/connection.py:101\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python_env/lib/python3.12/site-packages/httpcore/_sync/http11.py:143\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/anaconda3/envs/python_env/lib/python3.12/site-packages/httpcore/_sync/http11.py:113\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m    106\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    107\u001b[0m     (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m         trailing_data,\n\u001b[0;32m--> 113\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    115\u001b[0m         http_version,\n\u001b[1;32m    116\u001b[0m         status,\n\u001b[1;32m    117\u001b[0m         reason_phrase,\n\u001b[1;32m    118\u001b[0m         headers,\n\u001b[1;32m    119\u001b[0m     )\n\u001b[1;32m    121\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m~/anaconda3/envs/python_env/lib/python3.12/site-packages/httpcore/_sync/http11.py:186\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    183\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 186\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/python_env/lib/python3.12/site-packages/httpcore/_sync/http11.py:224\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    221\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 224\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/anaconda3/envs/python_env/lib/python3.12/site-packages/httpcore/_backends/sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python_env/lib/python3.12/ssl.py:1232\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1228\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1229\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1230\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1231\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1233\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[0;32m~/anaconda3/envs/python_env/lib/python3.12/ssl.py:1105\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1103\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1104\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1105\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[1;32m   1107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "groq_logical_five = groq_line_generate(raw_dataset = logical_deduction_five, output_dataset = groq_logical_five, start_idx=0, end_idx=9, system_message=\"You only answer A, B to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_logical_five = groq_line_generate(raw_dataset = logical_deduction_five, output_dataset = groq_logical_five, start_idx=10, end_idx=19, system_message=\"You only answer A, B to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_logical_five = groq_line_generate(raw_dataset = logical_deduction_five, output_dataset = groq_logical_five, start_idx=20, end_idx=29, system_message=\"You only answer A, B to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_logical_five = groq_line_generate(raw_dataset = logical_deduction_five, output_dataset = groq_logical_five, start_idx=30, end_idx=39, system_message=\"You only answer A, B to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_logical_five = groq_line_generate(raw_dataset = logical_deduction_five, output_dataset = groq_logical_five, start_idx=40, end_idx=49, system_message=\"You only answer A, B to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_logical_five = groq_line_generate(raw_dataset = logical_deduction_five, output_dataset = groq_logical_five, start_idx=50, end_idx=59, system_message=\"You only answer A, B to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_logical_five = groq_line_generate(raw_dataset = logical_deduction_five, output_dataset = groq_logical_five, start_idx=60, end_idx=69, system_message=\"You only answer A, B to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_logical_five = groq_line_generate(raw_dataset = logical_deduction_five, output_dataset = groq_logical_five, start_idx=70, end_idx=79, system_message=\"You only answer A, B to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_logical_five = groq_line_generate(raw_dataset = logical_deduction_five, output_dataset = groq_logical_five, start_idx=80, end_idx=89, system_message=\"You only answer A, B to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_logical_five = groq_line_generate(raw_dataset = logical_deduction_five, output_dataset = groq_logical_five, start_idx=90, end_idx=99, system_message=\"You only answer A, B to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_logical_five = groq_line_generate(raw_dataset = logical_deduction_five, output_dataset = groq_logical_five, start_idx=100, end_idx=109, system_message=\"You only answer A, B to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_logical_five = groq_line_generate(raw_dataset = logical_deduction_five, output_dataset = groq_logical_five, start_idx=110, end_idx=119, system_message=\"You only answer A, B to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_logical_five = groq_line_generate(raw_dataset = logical_deduction_five, output_dataset = groq_logical_five, start_idx=120, end_idx=129, system_message=\"You only answer A, B to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_logical_five = groq_line_generate(raw_dataset = logical_deduction_five, output_dataset = groq_logical_five, start_idx=130, end_idx=139, system_message=\"You only answer A, B to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_logical_five = groq_line_generate(raw_dataset = logical_deduction_five, output_dataset = groq_logical_five, start_idx=140, end_idx=149, system_message=\"You only answer A, B to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_logical_five = groq_line_generate(raw_dataset = logical_deduction_five, output_dataset = groq_logical_five, start_idx=150, end_idx=159, system_message=\"You only answer A, B to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_logical_five = groq_line_generate(raw_dataset = logical_deduction_five, output_dataset = groq_logical_five, start_idx=160, end_idx=169, system_message=\"You only answer A, B to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_logical_five = groq_line_generate(raw_dataset = logical_deduction_five, output_dataset = groq_logical_five, start_idx=170, end_idx=179, system_message=\"You only answer A, B to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_logical_five = groq_line_generate(raw_dataset = logical_deduction_five, output_dataset = groq_logical_five, start_idx=180, end_idx=189, system_message=\"You only answer A, B to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_logical_five = groq_line_generate(raw_dataset = logical_deduction_five, output_dataset = groq_logical_five, start_idx=190, end_idx=199, system_message=\"You only answer A, B to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_logical_five = groq_line_generate(raw_dataset = logical_deduction_five, output_dataset = groq_logical_five, start_idx=200, end_idx=209, system_message=\"You only answer A, B to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_logical_five = groq_line_generate(raw_dataset = logical_deduction_five, output_dataset = groq_logical_five, start_idx=210, end_idx=219, system_message=\"You only answer A, B to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_logical_five = groq_line_generate(raw_dataset = logical_deduction_five, output_dataset = groq_logical_five, start_idx=220, end_idx=229, system_message=\"You only answer A, B to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_logical_five = groq_line_generate(raw_dataset = logical_deduction_five, output_dataset = groq_logical_five, start_idx=230, end_idx=239, system_message=\"You only answer A, B to the following questions.\", model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_logical_five = groq_line_generate(raw_dataset = logical_deduction_five, output_dataset = groq_logical_five, start_idx=240, end_idx=249, system_message=\"You only answer A, B to the following questions.\", model_name=\"llama-3.3-70b-versatile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 250 entries, 0 to 249\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   input         250 non-null    object\n",
      " 1   target        250 non-null    object\n",
      " 2   llama_output  178 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 6.0+ KB\n"
     ]
    }
   ],
   "source": [
    "groq_logical_five.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Export failed: DataFrame contains null values in 'gemini_output'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_to_csv(\"Big Bench Hard\", groq_logical_five, file_name = \"groq_logical_five\", output_col = \"llama_output\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['A', 'B', 'A.', 'C', None, 'D',\n",
       "       'B is incorrect, E is the correct statement according to the text, but since I can only answer A or B, I will say: \\n\\nB',\n",
       "       'B.', nan], dtype=object)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groq_logical_five['llama_output'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_logical_five.loc[groq_logical_five['llama_output']== 'B.',\"llama_output\"] = \"B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['A', 'B', 'A.', 'C', None, 'D', nan], dtype=object)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groq_logical_five['llama_output'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>target</th>\n",
       "      <th>llama_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>The following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph. In an antique car show, there are five vehicles: a sedan, a convertible, a station wagon, a bus, and a motorcyle. The sedan is the second-newest. The station wagon is newer than the convertible. The sedan is older than the motorcyle. The bus is the oldest.\\nOptions:\\n(A) The sedan is the oldest\\n(B) The convertible is the oldest\\n(C) The station wagon is the oldest\\n(D) The bus is the oldest\\n(E) The motorcyle is the oldest</td>\n",
       "      <td>(D)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>The following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a branch, there are five birds: a falcon, a raven, a cardinal, a hawk, and a crow. The cardinal is the second from the right. The hawk is to the right of the falcon. The raven is the second from the left. The crow is the rightmost.\\nOptions:\\n(A) The falcon is the leftmost\\n(B) The raven is the leftmost\\n(C) The cardinal is the leftmost\\n(D) The hawk is the leftmost\\n(E) The crow is the leftmost</td>\n",
       "      <td>(A)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>The following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a shelf, there are five books: a black book, a purple book, a yellow book, an orange book, and a red book. The yellow book is the rightmost. The black book is to the left of the orange book. The orange book is to the left of the purple book. The black book is the second from the left.\\nOptions:\\n(A) The black book is the rightmost\\n(B) The purple book is the rightmost\\n(C) The yellow book is the rightmost\\n(D) The orange book is the rightmost\\n(E) The red book is the rightmost</td>\n",
       "      <td>(C)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>The following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a branch, there are five birds: a crow, a robin, a quail, a blue jay, and a falcon. The robin is to the left of the quail. The falcon is the third from the left. The crow is to the left of the falcon. The blue jay is the leftmost.\\nOptions:\\n(A) The crow is the second from the left\\n(B) The robin is the second from the left\\n(C) The quail is the second from the left\\n(D) The blue jay is the second from the left\\n(E) The falcon is the second from the left</td>\n",
       "      <td>(A)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>The following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph. A fruit stand sells five fruits: loquats, peaches, pears, plums, and watermelons. The watermelons are the most expensive. The peaches are more expensive than the loquats. The plums are the second-cheapest. The pears are the third-most expensive.\\nOptions:\\n(A) The loquats are the cheapest\\n(B) The peaches are the cheapest\\n(C) The pears are the cheapest\\n(D) The plums are the cheapest\\n(E) The watermelons are the cheapest</td>\n",
       "      <td>(A)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>The following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph. In a golf tournament, there were five golfers: Ana, Amy, Dan, Mya, and Eve. Ana finished below Mya. Dan finished above Amy. Mya finished second-to-last. Eve finished below Amy.\\nOptions:\\n(A) Ana finished first\\n(B) Amy finished first\\n(C) Dan finished first\\n(D) Mya finished first\\n(E) Eve finished first</td>\n",
       "      <td>(C)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>The following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a shelf, there are five books: a white book, an orange book, a yellow book, a blue book, and a red book. The yellow book is to the left of the white book. The red book is to the right of the blue book. The yellow book is to the right of the orange book. The blue book is to the right of the white book.\\nOptions:\\n(A) The white book is the third from the left\\n(B) The orange book is the third from the left\\n(C) The yellow book is the third from the left\\n(D) The blue book is the third from the left\\n(E) The red book is the third from the left</td>\n",
       "      <td>(A)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>The following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph. A fruit stand sells five fruits: watermelons, oranges, loquats, plums, and kiwis. The plums are less expensive than the kiwis. The plums are more expensive than the watermelons. The loquats are more expensive than the kiwis. The oranges are the most expensive.\\nOptions:\\n(A) The watermelons are the third-most expensive\\n(B) The oranges are the third-most expensive\\n(C) The loquats are the third-most expensive\\n(D) The plums are the third-most expensive\\n(E) The kiwis are the third-most expensive</td>\n",
       "      <td>(E)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>The following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a branch, there are five birds: a hawk, a raven, a robin, a hummingbird, and a crow. The robin is the leftmost. The raven is the second from the left. The hawk is the second from the right. The crow is the third from the left.\\nOptions:\\n(A) The hawk is the third from the left\\n(B) The raven is the third from the left\\n(C) The robin is the third from the left\\n(D) The hummingbird is the third from the left\\n(E) The crow is the third from the left</td>\n",
       "      <td>(E)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>The following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a branch, there are five birds: a hawk, a raven, a robin, a hummingbird, and a crow. The robin is the leftmost. The raven is the second from the left. The hawk is the second from the right. The crow is the third from the left.\\nOptions:\\n(A) The hawk is the second from the left\\n(B) The raven is the second from the left\\n(C) The robin is the second from the left\\n(D) The hummingbird is the second from the left\\n(E) The crow is the second from the left</td>\n",
       "      <td>(B)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            input  \\\n",
       "68                                                                                                                             The following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph. In an antique car show, there are five vehicles: a sedan, a convertible, a station wagon, a bus, and a motorcyle. The sedan is the second-newest. The station wagon is newer than the convertible. The sedan is older than the motorcyle. The bus is the oldest.\\nOptions:\\n(A) The sedan is the oldest\\n(B) The convertible is the oldest\\n(C) The station wagon is the oldest\\n(D) The bus is the oldest\\n(E) The motorcyle is the oldest   \n",
       "96                                                                                                                                                       The following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a branch, there are five birds: a falcon, a raven, a cardinal, a hawk, and a crow. The cardinal is the second from the right. The hawk is to the right of the falcon. The raven is the second from the left. The crow is the rightmost.\\nOptions:\\n(A) The falcon is the leftmost\\n(B) The raven is the leftmost\\n(C) The cardinal is the leftmost\\n(D) The hawk is the leftmost\\n(E) The crow is the leftmost   \n",
       "112                                                                   The following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a shelf, there are five books: a black book, a purple book, a yellow book, an orange book, and a red book. The yellow book is the rightmost. The black book is to the left of the orange book. The orange book is to the left of the purple book. The black book is the second from the left.\\nOptions:\\n(A) The black book is the rightmost\\n(B) The purple book is the rightmost\\n(C) The yellow book is the rightmost\\n(D) The orange book is the rightmost\\n(E) The red book is the rightmost   \n",
       "113                                                                                          The following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a branch, there are five birds: a crow, a robin, a quail, a blue jay, and a falcon. The robin is to the left of the quail. The falcon is the third from the left. The crow is to the left of the falcon. The blue jay is the leftmost.\\nOptions:\\n(A) The crow is the second from the left\\n(B) The robin is the second from the left\\n(C) The quail is the second from the left\\n(D) The blue jay is the second from the left\\n(E) The falcon is the second from the left   \n",
       "115                                                                                                                              The following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph. A fruit stand sells five fruits: loquats, peaches, pears, plums, and watermelons. The watermelons are the most expensive. The peaches are more expensive than the loquats. The plums are the second-cheapest. The pears are the third-most expensive.\\nOptions:\\n(A) The loquats are the cheapest\\n(B) The peaches are the cheapest\\n(C) The pears are the cheapest\\n(D) The plums are the cheapest\\n(E) The watermelons are the cheapest   \n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            ...   \n",
       "245                                                                                                                                                                                                                                                     The following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph. In a golf tournament, there were five golfers: Ana, Amy, Dan, Mya, and Eve. Ana finished below Mya. Dan finished above Amy. Mya finished second-to-last. Eve finished below Amy.\\nOptions:\\n(A) Ana finished first\\n(B) Amy finished first\\n(C) Dan finished first\\n(D) Mya finished first\\n(E) Eve finished first   \n",
       "246  The following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a shelf, there are five books: a white book, an orange book, a yellow book, a blue book, and a red book. The yellow book is to the left of the white book. The red book is to the right of the blue book. The yellow book is to the right of the orange book. The blue book is to the right of the white book.\\nOptions:\\n(A) The white book is the third from the left\\n(B) The orange book is the third from the left\\n(C) The yellow book is the third from the left\\n(D) The blue book is the third from the left\\n(E) The red book is the third from the left   \n",
       "247                                                   The following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph. A fruit stand sells five fruits: watermelons, oranges, loquats, plums, and kiwis. The plums are less expensive than the kiwis. The plums are more expensive than the watermelons. The loquats are more expensive than the kiwis. The oranges are the most expensive.\\nOptions:\\n(A) The watermelons are the third-most expensive\\n(B) The oranges are the third-most expensive\\n(C) The loquats are the third-most expensive\\n(D) The plums are the third-most expensive\\n(E) The kiwis are the third-most expensive   \n",
       "248                                                                                                  The following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a branch, there are five birds: a hawk, a raven, a robin, a hummingbird, and a crow. The robin is the leftmost. The raven is the second from the left. The hawk is the second from the right. The crow is the third from the left.\\nOptions:\\n(A) The hawk is the third from the left\\n(B) The raven is the third from the left\\n(C) The robin is the third from the left\\n(D) The hummingbird is the third from the left\\n(E) The crow is the third from the left   \n",
       "249                                                                                             The following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a branch, there are five birds: a hawk, a raven, a robin, a hummingbird, and a crow. The robin is the leftmost. The raven is the second from the left. The hawk is the second from the right. The crow is the third from the left.\\nOptions:\\n(A) The hawk is the second from the left\\n(B) The raven is the second from the left\\n(C) The robin is the second from the left\\n(D) The hummingbird is the second from the left\\n(E) The crow is the second from the left   \n",
       "\n",
       "    target llama_output  \n",
       "68     (D)         None  \n",
       "96     (A)         None  \n",
       "112    (C)         None  \n",
       "113    (A)         None  \n",
       "115    (A)         None  \n",
       "..     ...          ...  \n",
       "245    (C)          NaN  \n",
       "246    (A)          NaN  \n",
       "247    (E)          NaN  \n",
       "248    (E)          NaN  \n",
       "249    (B)          NaN  \n",
       "\n",
       "[72 rows x 3 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groq_logical_five[groq_logical_five['llama_output'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame exported successfully to /Users/ezishr/Library/CloudStorage/OneDrive-UniversityofCincinnati/Undergraduate Research/Check points/Big Bench Hard/groq_logical_five_not_done.csv!\n"
     ]
    }
   ],
   "source": [
    "# Export temporary GROQ generation DataFrame to CSV\n",
    "\n",
    "base_path = r'/Users/ezishr/Library/CloudStorage/OneDrive-UniversityofCincinnati/Undergraduate Research/Check points'\n",
    "file_path = os.path.join(base_path, \"Big Bench Hard\", \"groq_logical_five_not_done.csv\")\n",
    "os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "groq_logical_five.to_csv(file_path, index=False)\n",
    "print(f\"DataFrame exported successfully to {file_path}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = groq_logical_five[groq_logical_five['llama_output'].isnull()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[68, 96, 112, 113, 115, 118, 120, 122, 125, 127, 129, 135, 137, 140, 141, 142, 146, 148, 149, 151, 152, 155, 158, 159, 161, 162, 163, 164, 165, 166, 168, 169, 170, 175, 178, 203, 206, 208, 210, 214, 216, 218, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249]\n"
     ]
    }
   ],
   "source": [
    "indices = sample.index.tolist()\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in indices:\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\":\"user\",\n",
    "                \"content\": sample.loc[idx, 'input']\n",
    "            },\n",
    "            {\n",
    "                'role': 'system',\n",
    "                'content': \"You only answer A, B to the following questions.\"\n",
    "            }\n",
    "        ],\n",
    "        model = \"llama-3.3-70b-versatile\"\n",
    "    )\n",
    "\n",
    "    response = chat_completion.choices[0].message.content\n",
    "    sample.loc[idx, 'llama_output'] = response\n",
    "\n",
    "\n",
    "\n",
    "### UPDATE THE GROQ RATE LIMIT - FEB 01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Testing - logical_deduction_seven"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "logical_deduction_seven = Big_Bench_Json_Processor_my('Big Bench Hard', 'logical_deduction_seven_objects.json').convert_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logical_deduction_seven.head()\n",
    "logical_deduction_seven['target'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logical_deduction_seven.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GEMINI Applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_logical_deduction_seven = gemini_generator(logical_deduction_seven, \"You only answer A, B, C, D, E to the following questions.\")\n",
    "df_to_csv(\"Big Bench Hard\", gemini_logical_deduction_seven, file_name = \"gemini_logical_deduction_seven\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Testing - logical_deduction_three"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logical_deduction_three = Big_Bench_Json_Processor_my('Big Bench Hard', 'logical_deduction_three_objects.json').convert_df()\n",
    "logical_deduction_three.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logical_deduction_three.info()\n",
    "print(logical_deduction_three['target'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GEMINI Applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_logical_deduction_three = gemini_generator(logical_deduction_three, \"You only answer A, B, C  to the following questions.\")\n",
    "\n",
    "df_to_csv(\"Big Bench Hard\", gemini_logical_deduction_three, file_name = \"gemini_logi cal_deduction_three\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Testing - movie_recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_recommendation = Big_Bench_Json_Processor_my('Big Bench Hard', 'movie_recommendation.json').convert_df()\n",
    "\n",
    "movie_recommendation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_recommendation['target'].unique() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_recommendation[movie_recommendation['target']=='Monsters, Inc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GEMINI Applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_movie_recommendation = gemini_generator(movie_recommendation, \"Your answers to the following question should be in the provided options.\")\n",
    "\n",
    "df_to_csv(\"Big Bench Hard\", gemini_movie_recommendation, file_name = \"gemini_movie_recommendation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Testing - multistep_arithmetic_two"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multistep_arithmetic_two = Big_Bench_Json_Processor_my('Big Bench Hard', 'multistep_arithmetic_two.json').convert_df()\n",
    "multistep_arithmetic_two.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multistep_arithmetic_two['target'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multistep_arithmetic_two.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GEMINI Applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_multistep_arithmetic_two = gemini_generator(multistep_arithmetic_two, \"Your answers should only be numbers to the following questions.\")\n",
    "\n",
    "df_to_csv(\"Big Bench Hard\", gemini_multistep_arithmetic_two, file_name = \"gemini_multistep_arithmetic_two\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Testing - navigate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "navigate = Big_Bench_Json_Processor_my('Big Bench Hard', 'navigate.json').convert_df()\n",
    "navigate.info()\n",
    "navigate.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "navigate['target'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GEMINI Applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_navigate = gemini_generator(navigate, \"You only answer Yes of No to the following questions.\")\n",
    "\n",
    "df_to_csv(\"Big Bench Hard\", gemini_navigate, file_name = \"gemini_navigate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My testing - object_counting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_counting = Big_Bench_Json_Processor_my('Big Bench Hard', 'object_counting.json').convert_df()\n",
    "object_counting.info()\n",
    "object_counting['target'].unique()\n",
    "object_counting.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GEMINI Applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_object_counting = gemini_generator(object_counting, \"You only answer the number of objects to the following questions.\")\n",
    "\n",
    "df_to_csv(\"Big Bench Hard\", gemini_object_counting, file_name = \"gemini_object_counting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_counting.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=genai.GenerativeModel(\n",
    "  model_name=\"gemini-1.5-flash\",\n",
    "  system_instruction=\"You only answer True or False to the following questions.\",)\n",
    "\n",
    "model.generate_content(object_counting['input'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Testing - penguins_in_a_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins_in_a_table = Big_Bench_Json_Processor_my('Big Bench Hard', 'penguins_in_a_table.json').convert_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins_in_a_table.info()\n",
    "print(penguins_in_a_table['target'].unique())\n",
    "penguins_in_a_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GEMINI Applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_penguins_in_a_table = gemini_generator(penguins_in_a_table, \"You only answer the number of penguins to the following questions.\")\n",
    "\n",
    "df_to_csv(\"Big Bench Hard\", gemini_penguins_in_a_table, file_name = \"gemini_penguins_in_a_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Testing - reasoning_about_colored_objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_colored_objects = Big_Bench_Json_Processor_my('Big Bench Hard', 'reasoning_about_colored_objects.json').convert_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reasoning_colored_objects.info())\n",
    "print(reasoning_colored_objects['target'].unique())\n",
    "reasoning_colored_objects.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GEMINI Applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_reasoning_colored_objects = gemini_generator(reasoning_colored_objects, \"You only choose the options provided to answer the following questions.\")\n",
    "df_to_csv(\"Big Bench Hard\", gemini_reasoning_colored_objects, file_name = \"gemini_reasoning_colored_objects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Testing - ruin_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruin_names = Big_Bench_Json_Processor_my('Big Bench Hard', 'ruin_names.json').convert_df()\n",
    "print(ruin_names.info())\n",
    "print(ruin_names['target'].unique())\n",
    "ruin_names.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruin_names[ruin_names['target'].isin(['dearth, wind, & fire', 'rita, sue and bob poo'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GEMINI Applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_ruin_names = gemini_generator(ruin_names, \"You only answer the options provided to answer the following questions. You can choose multiple options.\")\n",
    "df_to_csv(\"Big Bench Hard\", gemini_ruin_names, file_name = \"gemini_ruin_names\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Testing - salient_translation_error_detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salient_detection = Big_Bench_Json_Processor_my('Big Bench Hard', 'salient_translation_error_detection.json').convert_df()\n",
    "print(salient_detection.info())\n",
    "print(salient_detection['target'].unique())\n",
    "salient_detection.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(salient_detection['input'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GEMINI Applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_salient_detection = gemini_generator(salient_detection, \"You only answer the options provided to answer the following questions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_csv(\"Big Bench Hard\", gemini_salient_detection, file_name = \"gemini_salient_detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Testing - snarks.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snarks = Big_Bench_Json_Processor_my('Big Bench Hard', 'snarks.json').convert_df()\n",
    "print(snarks.info())\n",
    "print(snarks['target'].unique())\n",
    "snarks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GEMINI Applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_snarks = gemini_generator(snarks, \"You only answer the options provided to answer the following questions.\")\n",
    "df_to_csv(\"Big Bench Hard\", gemini_snarks, file_name = \"gemini_snarks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Testing - sports_understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sports_understanding = Big_Bench_Json_Processor_my('Big Bench Hard', 'sports_understanding.json').convert_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sports_understanding.info())\n",
    "print(sports_understanding['target'].unique())\n",
    "sports_understanding.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GEMINI Applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_sports_understanding = gemini_generator(sports_understanding, \"You only Yes or No to the following questions.\")\n",
    "df_to_csv(\"Big Bench Hard\", gemini_sports_understanding, file_name = \"gemini_sports_understanding\", output_col=\"gemini_output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GROQ - llama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sports_understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample5_10 = sports_understanding.head(10)\n",
    "groq_sample = groq(sample5_10, \"You only Yes or No to the following questions.\", \"llama-3.3-70b-versatile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_sample.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_sports_understanding = sports_understanding.copy()\n",
    "groq_sports_understanding['llama_output'] = groq_sample[\"llama-3.3-70b-versatile\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_sports_understanding.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sports_understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_10_19 = sports_understanding.loc[10:20, ].copy()\n",
    "sample_10_19.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_sample = groq(sample_10_19, \"You only Yes or No to the following questions.\", \"llama-3.3-70b-versatile\")\n",
    "groq_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_sports_understanding.loc[10:19, \"llama_output\"] = groq_sample[\"llama-3.3-70b-versatile\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_sports_understanding.loc[10:19, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_sports_understanding.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_20_29 = sports_understanding.loc[20:29, ].copy()\n",
    "sample_20_29.reset_index(drop=True, inplace=True)\n",
    "groq_sample = groq(sample_20_29, \"You only Yes or No to the following questions.\", \"llama-3.3-70b-versatile\")\n",
    "groq_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_sports_understanding.loc[20:29, \"llama_output\"] = groq_sample[\"llama-3.3-70b-versatile\"].values\n",
    "groq_sports_understanding.loc[20:29, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_sports_understanding = groq_line_generate(sports_understanding, groq_sports_understanding, 30, 39, \"You only Yes or No to the following questions.\", \"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_sports_understanding = groq_line_generate(sports_understanding, groq_sports_understanding, 40, 49, \"You only Yes or No to the following questions.\", \"llama-3.3-70b-versatile\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_sports_understanding = groq_line_generate(sports_understanding, groq_sports_understanding, 50, 59, \"You only Yes or No to the following questions.\", \"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_sports_understanding = groq_line_generate(sports_understanding, groq_sports_understanding, 60, 69, \"You only Yes or No to the following questions.\", \"llama-3.3-70b-versatile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_sports_understanding = groq_line_generate(sports_understanding, groq_sports_understanding, 70, 79, \"You only Yes or No to the following questions.\", \"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_sports_understanding = groq_line_generate(sports_understanding, groq_sports_understanding, 80, 89, \"You only Yes or No to the following questions.\", \"llama-3.3-70b-versatile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_sports_understanding = groq_line_generate(sports_understanding, groq_sports_understanding, 90, 99, \"You only Yes or No to the following questions.\", \"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_sports_understanding = groq_line_generate(sports_understanding, groq_sports_understanding, 90, 109, \"You only Yes or No to the following questions.\", \"llama-3.3-70b-versatile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_sports_understanding = groq_line_generate(sports_understanding, groq_sports_understanding, 110, 119, \"You only Yes or No to the following questions.\", \"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_sports_understanding = groq_line_generate(sports_understanding, groq_sports_understanding, 120, 129, \"You only Yes or No to the following questions.\", \"llama-3.3-70b-versatile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_sports_understanding = groq_line_generate(sports_understanding, groq_sports_understanding, 130, 139, \"You only Yes or No to the following questions.\", \"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_sports_understanding = groq_line_generate(sports_understanding, groq_sports_understanding, 140, 149, \"You only Yes or No to the following questions.\", \"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_sports_understanding = groq_line_generate(sports_understanding, groq_sports_understanding, 150, 159, \"You only Yes or No to the following questions.\", \"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_sports_understanding = groq_line_generate(sports_understanding, groq_sports_understanding, 160, 169, \"You only Yes or No to the following questions.\", \"llama-3.3-70b-versatile\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_sports_understanding = groq_line_generate(sports_understanding, groq_sports_understanding, 170, 179, \"You only Yes or No to the following questions.\", \"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_sports_understanding = groq_line_generate(sports_understanding, groq_sports_understanding, 180, 189, \"You only Yes or No to the following questions.\", \"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_sports_understanding = groq_line_generate(sports_understanding, groq_sports_understanding, 190, 199, \"You only Yes or No to the following questions.\", \"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_sports_understanding = groq_line_generate(sports_understanding, groq_sports_understanding, 200, 209, \"You only Yes or No to the following questions.\", \"llama-3.3-70b-versatile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_sports_understanding = groq_line_generate(sports_understanding, groq_sports_understanding, 210, 219, \"You only Yes or No to the following questions.\", \"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_sports_understanding = groq_line_generate(sports_understanding, groq_sports_understanding, 220, 229, \"You only Yes or No to the following questions.\", \"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_sports_understanding = groq_line_generate(sports_understanding, groq_sports_understanding, 230, 239, \"You only Yes or No to the following questions.\", \"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_sports_understanding = groq_line_generate(sports_understanding, groq_sports_understanding, 240, 250, \"You only Yes or No to the following questions.\", \"llama-3.3-70b-versatile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_sports_understanding.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_csv(\"Big Bench Hard\", groq_sports_understanding, file_name = \"groq_sports_understanding\", output_col=\"llama_output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Groq(api_key=os.environ['GROQ_API_KEY'],)\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\":\"user\",\n",
    "            \"content\": groq_causal_judgement.loc[188, 'input']\n",
    "        },\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': \"You only Yes or No to the following questions.\"\n",
    "        }\n",
    "    ],\n",
    "    model = \"llama-3.3-70b-versatile\"\n",
    ")\n",
    "\n",
    "response = chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_causal_judgement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Testing = temporal_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_sequences = Big_Bench_Json_Processor_my('Big Bench Hard', 'temporal_sequences.json').convert_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_sequences.info()\n",
    "temporal_sequences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GEMINI Applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_temporal_sequences = gemini_generator(temporal_sequences, \"You only answer the options provided to the following questions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_csv(\"Big Bench Hard\", gemini_temporal_sequences, file_name = \"gemini_temporal_sequences\", output_col=\"gemini_output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Testing - tracking_shuffled_objects_five"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracking_five = Big_Bench_Json_Processor_my('Big Bench Hard', 'tracking_shuffled_objects_five_objects.json').convert_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracking_five.info()\n",
    "tracking_five.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GEMINI Applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_tracking_five = gemini_generator(tracking_five, \"You only answer the options provided to the following questions.\")\n",
    "df_to_csv(\"Big Bench Hard\", gemini_tracking_five, file_name = \"gemini_tracking_five\", output_col=\"gemini_output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Testing - tracking_shuffled_objects_seven"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracking_seven = Big_Bench_Json_Processor_my('Big Bench Hard', 'tracking_shuffled_objects_seven_objects.json').convert_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracking_seven.info()\n",
    "tracking_seven.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GEMINI Applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_tracking_seven = gemini_generator(tracking_seven, \"You only answer the options provided to the following questions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_csv(df = gemini_tracking_seven, folder_name = \"Big Bench Hard\", file_name = \"gemini_tracking_seven\", output_col=\"gemini_output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Testing - tracking_shuffled_objects_three"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_three = Big_Bench_Json_Processor_my('Big Bench Hard', 'tracking_shuffled_objects_three_objects.json').convert_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_three.info()\n",
    "track_three.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GEMINI Applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_track_three = gemini_generator(track_three, \"You only answer A, B, C,to the following questions.\")\n",
    "df_to_csv(\"Big Bench Hard\", gemini_track_three, file_name = \"gemini_track_three\", output_col=\"gemini_output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Testing - web_of_lies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_of_lies = Big_Bench_Json_Processor_my('Big Bench Hard', 'web_of_lies.json').convert_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_of_lies.info()\n",
    "print(web_of_lies['target'].unique())\n",
    "web_of_lies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GEMINI Applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_web_of_lies = gemini_generator(web_of_lies, \"You only answer Yes or No to the following questions.\")\n",
    "df_to_csv(\"Big Bench Hard\", gemini_web_of_lies, file_name = \"gemini_web_of_lies\", output_col=\"gemini_output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GROQ - llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_web_of_lies = web_of_lies.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_web_of_lies['llama_output'] = groq_line_generate(web_of_lies, groq_web_of_lies, 0, 9, \"You only answer Yes or No to the following questions.\", \"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_web_of_lies['llama_output'] = groq_line_generate(web_of_lies, groq_web_of_lies, 10, 19, \"You only answer Yes or No to the following questions.\", \"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_web_of_lies['llama_output'] = groq_line_generate(web_of_lies, groq_web_of_lies, 20, 29, \"You only answer Yes or No to the following questions.\", \"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_web_of_lies['llama_output'] = groq_line_generate(web_of_lies, groq_web_of_lies, 30, 39, \"You only answer Yes or No to the following questions.\", \"llama-3.3-70b-versatile\")\n",
    "\n",
    "groq_web_of_lies['llama_output'] = groq_line_generate(web_of_lies, groq_web_of_lies, 40, 49, \"You only answer Yes or No to the following questions.\", \"llama-3.3-70b-versatile\")[\"llama-3.3-70b-versatile\"]\n",
    "\n",
    "groq_web_of_lies['llama_output'] = groq_line_generate(web_of_lies, groq_web_of_lies, 50, 59, \"You only answer Yes or No to the following questions.\", \"llama-3.3-70b-versatile\")[\"llama-3.3-70b-versatile\"]\n",
    "\n",
    "groq_web_of_lies['llama_output'] = groq_line_generate(web_of_lies, groq_web_of_lies, 60, 69, \"You only answer Yes or No to the following questions.\", \"llama-3.3-70b-versatile\")[\"llama-3.3-70b-versatile\"]\n",
    "\n",
    "groq_web_of_lies['llama_output'] = groq_line_generate(web_of_lies, groq_web_of_lies, 70, 79, \"You only answer Yes or No to the following questions.\", \"llama-3.3-70b-versatile\")[\"llama-3.3-70b-versatile\"]\n",
    "\n",
    "groq_web_of_lies['llama_output'] = groq_line_generate(web_of_lies, groq_web_of_lies, 80, 89, \"You only answer Yes or No to the following questions.\", \"llama-3.3-70b-versatile\")[\"llama-3.3-70b-versatile\"]\n",
    "\n",
    "groq_web_of_lies['llama_output'] = groq_line_generate(web_of_lies, groq_web_of_lies, 90, 99, \"You only answer Yes or No to the following questions.\", \"llama-3.3-70b-versatile\")[\"llama-3.3-70b-versatile\"]\n",
    "\n",
    "groq_web_of_lies['llama_output'] = groq_line_generate(web_of_lies, groq_web_of_lies, 100, 109, \"You only answer Yes or No to the following questions.\", \"llama-3.3-70b-versatile\")[\"llama-3.3-70b-versatile\"]\n",
    "\n",
    "groq_web_of_lies['llama_output'] = groq_line_generate(web_of_lies, groq_web_of_lies, 110, 119, \"You only answer Yes or No to the following questions.\", \"llama-3.3-70b-versatile\")[\"llama-3.3-70b-versatile\"]\n",
    "\n",
    "groq_web_of_lies['llama_output'] = groq_line_generate(web_of_lies, groq_web_of_lies, 120, 129, \"You only answer Yes or No to the following questions.\", \"llama-3.3-70b-versatile\")[\"llama-3.3-70b-versatile\"]\n",
    "\n",
    "groq_web_of_lies['llama_output'] = groq_line_generate(web_of_lies, groq_web_of_lies, 130, 139, \"You only answer Yes or No to the following questions.\", \"llama-3.3-70b-versatile\")[\"llama-3.3-70b-versatile\"]\n",
    "\n",
    "groq_web_of_lies['llama_output'] = groq_line_generate(web_of_lies, groq_web_of_lies, 140, 149, \"You only answer Yes or No to the following questions.\", \"llama-3.3-70b-versatile\")[\"llama-3.3-70b-versatile\"]\n",
    "\n",
    "groq_web_of_lies['llama_output'] = groq_line_generate(web_of_lies, groq_web_of_lies, 150, 159, \"You only answer Yes or No to the following questions.\", \"llama-3.3-70b-versatile\")[\"llama-3.3-70b-versatile\"]\n",
    "\n",
    "groq_web_of_lies['llama_output'] = groq_line_generate(web_of_lies, groq_web_of_lies, 160, 169, \"You only answer Yes or No to the following questions.\", \"llama-3.3-70b-versatile\")[\"llama-3.3-70b-versatile\"]\n",
    "\n",
    "groq_web_of_lies['llama_output'] = groq_line_generate(web_of_lies, groq_web_of_lies, 170, 179, \"You only answer Yes or No to the following questions.\", \"llama-3.3-70b-versatile\")[\"llama-3.3-70b-versatile\"]\n",
    "\n",
    "groq_web_of_lies['llama_output'] = groq_line_generate(web_of_lies, groq_web_of_lies, 180, 189, \"You only answer Yes or No to the following questions.\", \"llama-3.3-70b-versatile\")[\"llama-3.3-70b-versatile\"]\n",
    "\n",
    "groq_web_of_lies['llama_output'] = groq_line_generate(web_of_lies, groq_web_of_lies, 190, 199, \"You only answer Yes or No to the following questions.\", \"llama-3.3-70b-versatile\")[\"llama-3.3-70b-versatile\"]\n",
    "\n",
    "groq_web_of_lies['llama_output'] = groq_line_generate(web_of_lies, groq_web_of_lies, 200, 209, \"You only answer Yes or No to the following questions.\", \"llama-3.3-70b-versatile\")[\"llama-3.3-70b-versatile\"]\n",
    "\n",
    "groq_web_of_lies['llama_output'] = groq_line_generate(web_of_lies, groq_web_of_lies, 210, 219, \"You only answer Yes or No to the following questions.\", \"llama-3.3-70b-versatile\")[\"llama-3.3-70b-versatile\"]\n",
    "\n",
    "groq_web_of_lies['llama_output'] = groq_line_generate(web_of_lies, groq_web_of_lies, 220, 229, \"You only answer Yes or No to the following questions.\", \"llama-3.3-70b-versatile\")[\"llama-3.3-70b-versatile\"]\n",
    "\n",
    "groq_web_of_lies['llama_output'] = groq_line_generate(web_of_lies, groq_web_of_lies, 230, 239, \"You only answer Yes or No to the following questions.\", \"llama-3.3-70b-versatile\")[\"llama-3.3-70b-versatile\"]\n",
    "\n",
    "groq_web_of_lies['llama_output'] = groq_line_generate(web_of_lies, groq_web_of_lies, 240, 249, \"You only answer Yes or No to the following questions.\", \"llama-3.3-70b-versatile\")[\"llama-3.3-70b-versatile\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_web_of_lies.info()\n",
    "df_to_csv(\"Big Bench Hard\", groq_web_of_lies, file_name = \"groq_web_of_lies\", output_col=\"llama_output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Professor Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = os.listdir('../../Big Bench Hard/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_lst = [file for file in os.listdir('../../Big Bench Hard/data') if file.endswith(\"json\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('../../Big Bench Hard/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path = 'output.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_credit_card_number(card_number):\n",
    "    \"\"\"\n",
    "    Checks if a credit card number is valid using Luhn's Algorithm.\n",
    "\n",
    "    Args:\n",
    "        card_number: The credit card number as a string, with or without dashes.\n",
    "\n",
    "    Returns:\n",
    "        True if the card number is valid, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove dashes and spaces\n",
    "    card_number = card_number.replace(\"-\", \"\").replace(\" \", \"\")\n",
    "\n",
    "    # Check if the card number contains only digits and has a valid length\n",
    "    if not card_number.isdigit() or not (13 <= len(card_number) <= 19):  # Common card lengths\n",
    "        return False\n",
    "\n",
    "    # Reverse the card number\n",
    "    reversed_card_number = card_number[::-1]\n",
    "\n",
    "    # Calculate the sum according to Luhn's algorithm\n",
    "    total = 0\n",
    "    for i, digit in enumerate(reversed_card_number):\n",
    "        digit = int(digit)\n",
    "        if (i + 1) % 2 == 0:  # Double every second digit\n",
    "            doubled_digit = digit * 2\n",
    "            if doubled_digit > 9:\n",
    "                total += doubled_digit - 9  # Subtract 9 if the doubled digit is greater than 9\n",
    "            else:\n",
    "                total += doubled_digit\n",
    "        else:\n",
    "            total += digit\n",
    "\n",
    "    # Check if the total is divisible by 10\n",
    "    return total % 10 == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases \n",
    "print(check_credit_card_number(\"49927398716\")) # Valid \n",
    "print(check_credit_card_number(\"4992-7398-716\")) # Valid (with dashes) \n",
    "print(check_credit_card_number(\"4992 7398 716\")) # Valid (with spaces) \n",
    "print(check_credit_card_number(\"49927398717\")) # Invalid \n",
    "print(check_credit_card_number(\"1234567890123456\")) # Valid \n",
    "print(check_credit_card_number(\"1234-5678-9012-3457\")) # Invalid \n",
    "print(check_credit_card_number(\"abc1234567890\")) # Invalid (non-digit characters) \n",
    "print(check_credit_card_number(\"1234\")) # Invalid (too short) \n",
    "print(check_credit_card_number(\"1234567890123456789\")) # Invalid (too long)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
